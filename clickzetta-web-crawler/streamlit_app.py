"""
ClickZetta Web Crawler & Storage Demo

å±•ç¤ºLangChainç½‘ç«™çˆ¬å–åŠŸèƒ½ä¸ClickZettaå­˜å‚¨æœåŠ¡çš„å®Œæ•´é›†æˆã€‚
æ¼”ç¤ºæ–‡æ¡£å­˜å‚¨ã€é”®å€¼å­˜å‚¨ã€æ–‡ä»¶å­˜å‚¨å’Œå‘é‡å­˜å‚¨çš„å®é™…åº”ç”¨ã€‚
"""

import streamlit as st
import sys
import os
import hashlib
import json
import time
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from urllib.parse import urljoin, urlparse
import re

try:
    import requests
    from bs4 import BeautifulSoup
    import html2text
    import validators
    from langchain_core.documents import Document
    from langchain_community.embeddings import DashScopeEmbeddings
    from langchain_clickzetta import (
        ClickZettaEngine,
        ClickZettaStore,
        ClickZettaDocumentStore,
        ClickZettaFileStore,
        ClickZettaVectorStore
    )
    from dotenv import load_dotenv
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    DEPENDENCIES_AVAILABLE = False
    import_error = str(e)

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="ClickZetta Web Crawler & Storage Demo",
    page_icon="ğŸ•·ï¸",
    layout="wide"
)

def extract_text_content(html_content: str) -> str:
    """ä»HTMLä¸­æå–çº¯æ–‡æœ¬å†…å®¹"""
    if not DEPENDENCIES_AVAILABLE:
        return "ä¾èµ–æœªå®‰è£…"

    try:
        # ä½¿ç”¨html2textè½¬æ¢HTMLä¸ºMarkdownæ ¼å¼æ–‡æœ¬
        h = html2text.HTML2Text()
        h.ignore_links = False
        h.ignore_images = True
        h.ignore_emphasis = False
        text_content = h.handle(html_content)

        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        lines = [line.strip() for line in text_content.split('\n')]
        cleaned_lines = []
        for line in lines:
            if line or (cleaned_lines and cleaned_lines[-1]):
                cleaned_lines.append(line)

        return '\n'.join(cleaned_lines)
    except Exception as e:
        st.warning(f"æ–‡æœ¬æå–å¤±è´¥: {e}")
        # fallback to BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        return soup.get_text()

def extract_metadata(soup, url: str) -> Dict:
    """æå–ç½‘é¡µå…ƒæ•°æ®"""
    metadata = {
        "url": url,
        "crawled_at": datetime.now().isoformat(),
        "title": "",
        "description": "",
        "author": "",
        "keywords": [],
        "language": "zh",
        "content_type": "webpage"
    }

    try:
        # æå–æ ‡é¢˜
        title_tag = soup.find('title')
        if title_tag:
            metadata["title"] = title_tag.get_text().strip()

        # æå–æè¿°
        desc_tag = soup.find('meta', attrs={'name': 'description'})
        if desc_tag:
            metadata["description"] = desc_tag.get('content', '').strip()

        # æå–ä½œè€…
        author_tag = soup.find('meta', attrs={'name': 'author'})
        if author_tag:
            metadata["author"] = author_tag.get('content', '').strip()

        # æå–å…³é”®è¯
        keywords_tag = soup.find('meta', attrs={'name': 'keywords'})
        if keywords_tag:
            keywords = keywords_tag.get('content', '').strip()
            metadata["keywords"] = [k.strip() for k in keywords.split(',') if k.strip()]

        # æ£€æµ‹è¯­è¨€
        html_tag = soup.find('html')
        if html_tag and html_tag.get('lang'):
            metadata["language"] = html_tag.get('lang')

    except Exception as e:
        st.warning(f"å…ƒæ•°æ®æå–å¤±è´¥: {e}")

    return metadata

def url_to_key(url: str) -> str:
    """å°†URLè½¬æ¢ä¸ºå­˜å‚¨é”®"""
    return hashlib.md5(url.encode()).hexdigest()

def load_env_config():
    """åŠ è½½ç¯å¢ƒé…ç½®"""
    load_dotenv()
    return {
        'clickzetta_service': os.getenv('CLICKZETTA_SERVICE', ''),
        'clickzetta_instance': os.getenv('CLICKZETTA_INSTANCE', ''),
        'clickzetta_workspace': os.getenv('CLICKZETTA_WORKSPACE', ''),
        'clickzetta_schema': os.getenv('CLICKZETTA_SCHEMA', ''),
        'clickzetta_username': os.getenv('CLICKZETTA_USERNAME', ''),
        'clickzetta_password': os.getenv('CLICKZETTA_PASSWORD', ''),
        'clickzetta_vcluster': os.getenv('CLICKZETTA_VCLUSTER', ''),
        'dashscope_api_key': os.getenv('DASHSCOPE_API_KEY', ''),
    }

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒé…ç½®"""
    config = load_env_config()

    clickzetta_available = all([
        config['clickzetta_service'], config['clickzetta_instance'],
        config['clickzetta_workspace'], config['clickzetta_schema'],
        config['clickzetta_username'], config['clickzetta_password'],
        config['clickzetta_vcluster']
    ])

    dashscope_available = bool(config['dashscope_api_key'])

    return {
        "clickzetta_available": clickzetta_available,
        "dashscope_available": dashscope_available,
        "config": config
    }

def display_environment_status(env_status):
    """æ˜¾ç¤ºç¯å¢ƒçŠ¶æ€"""
    col1, col2 = st.columns(2)

    with col1:
        if env_status["clickzetta_available"]:
            st.success("âœ… ClickZetta é…ç½®å®Œæ•´")
        else:
            st.error("âŒ ClickZetta é…ç½®ä¸å®Œæ•´")

    with col2:
        if env_status["dashscope_available"]:
            st.success("âœ… DashScope API é…ç½®å®Œæ•´")
            st.session_state['dashscope_available'] = True
        else:
            st.warning("âš ï¸ DashScope API æœªé…ç½®")
            st.session_state['dashscope_available'] = False

def create_sidebar_info():
    """åˆ›å»ºä¾§è¾¹æ ä¿¡æ¯"""
    st.sidebar.header("â„¹ï¸ åº”ç”¨ä¿¡æ¯")
    st.sidebar.markdown("""
    ### ğŸ•·ï¸ ç½‘ç»œçˆ¬è™«æ¼”ç¤º

    **åŠŸèƒ½ç‰¹è‰²:**
    - ç½‘é¡µå†…å®¹çˆ¬å–
    - å¤šç§å­˜å‚¨æœåŠ¡
    - è¯­ä¹‰æœç´¢
    - ç»Ÿè®¡åˆ†æ

    **å­˜å‚¨æœåŠ¡:**
    - DocumentStore: æ–‡æ¡£å­˜å‚¨
    - Store: é”®å€¼å­˜å‚¨
    - FileStore: æ–‡ä»¶å­˜å‚¨
    - VectorStore: å‘é‡å­˜å‚¨
    """)

    # æ•°æ®ç®¡ç†åŠŸèƒ½
    st.sidebar.header("ğŸ—‘ï¸ æ•°æ®ç®¡ç†")

    # ç»Ÿè®¡ä¿¡æ¯
    with st.sidebar.expander("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯"):
        env_status = check_environment()
        if env_status["clickzetta_available"]:
            try:
                engine = get_clickzetta_engine()

                # æ£€æŸ¥å„ç§å­˜å‚¨çš„æ•°æ®é‡
                doc_count = 0
                cache_count = 0
                vector_count = 0
                file_count = 0

                try:
                    # DocumentStoreæ•°æ®
                    doc_result, _ = engine.execute_query("SELECT COUNT(*) as count FROM web_crawler_documents")
                    doc_count = doc_result[0]['count'] if doc_result else 0
                except:
                    pass

                try:
                    # Cacheæ•°æ®
                    cache_result, _ = engine.execute_query("SELECT COUNT(*) as count FROM web_crawler_cache")
                    cache_count = cache_result[0]['count'] if cache_result else 0
                except:
                    pass

                try:
                    # Vectoræ•°æ®
                    vector_result, _ = engine.execute_query("SELECT COUNT(*) as count FROM web_crawler_vectors")
                    vector_count = vector_result[0]['count'] if vector_result else 0
                except:
                    pass

                st.metric("ğŸ“š æ–‡æ¡£æ•°æ®", f"{doc_count} æ¡")
                st.metric("ğŸ—‚ï¸ ç¼“å­˜æ•°æ®", f"{cache_count} æ¡")
                st.metric("ğŸ§  å‘é‡æ•°æ®", f"{vector_count} æ¡")

                total_data = doc_count + cache_count + vector_count
                if total_data > 0:
                    st.success(f"ğŸ’¡ æ£€æµ‹åˆ°å·²æœ‰ {total_data} æ¡æ•°æ®")

            except Exception as e:
                st.warning(f"âš ï¸ æ— æ³•è·å–ç»Ÿè®¡ä¿¡æ¯: {e}")
        else:
            st.warning("âš ï¸ è¯·å…ˆé…ç½®ClickZettaè¿æ¥")

    # æ¸…ç©ºæ•°æ®åŠŸèƒ½
    with st.sidebar.expander("ğŸ—‘ï¸ æ•°æ®æ¸…ç©º"):
        st.write("**æ¸…ç©ºçˆ¬è™«æ•°æ®**")
        st.caption("åˆ é™¤æ‰€æœ‰çˆ¬å–çš„ç½‘é¡µæ•°æ®å’Œæ–‡ä»¶ï¼Œé‡æ–°å¼€å§‹")

        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ•°æ®", type="secondary", help="åˆ é™¤æ‰€æœ‰æ•°æ®"):
            env_status = check_environment()
            if env_status["clickzetta_available"]:
                st.session_state.clear_data_requested = True
                st.info("æ•°æ®æ¸…ç©ºè¯·æ±‚å·²æäº¤ï¼Œæ­£åœ¨å¤„ç†...")
                st.rerun()
            else:
                st.warning("âš ï¸ è¯·å…ˆé…ç½®ClickZettaè¿æ¥")

def clear_file_storage(crawler_instance):
    """æ¸…ç©ºæ–‡ä»¶å­˜å‚¨"""
    try:
        # è·å–æ‰€æœ‰æ–‡ä»¶
        files = crawler_instance.file_store.list_files()
        if not files:
            st.info("æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶éœ€è¦åˆ é™¤")
            return 0

        st.info(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹åˆ é™¤...")

        # æ”¶é›†æ–‡ä»¶è·¯å¾„
        file_paths = []
        for file_info in files:
            if isinstance(file_info, tuple) and len(file_info) >= 1:
                file_paths.append(file_info[0])

        if not file_paths:
            st.info("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„")
            return 0

        # åˆ é™¤æ–‡ä»¶

        # æ„å»ºæ­£ç¡®çš„é”®åï¼ˆåŸºäº store_file çš„å­˜å‚¨æ–¹å¼ï¼‰
        keys_to_delete = []
        for key in file_paths:
            keys_to_delete.append(key)                    # ä¸»æ–‡ä»¶é”®
            keys_to_delete.append(f"_metadata_{key}")     # å…ƒæ•°æ®é”®

        # ä½¿ç”¨ volume_store.mdelete åˆ é™¤
        crawler_instance.file_store.volume_store.mdelete(keys_to_delete)

        # éªŒè¯åˆ é™¤ç»“æœ
        remaining_files = crawler_instance.file_store.list_files()
        remaining_count = len(remaining_files) if remaining_files else 0

        if remaining_count == 0:
            st.success("ğŸ‰ æ‰€æœ‰æ–‡ä»¶å·²æ¸…ç©º")
            return len(file_paths)
        else:
            st.warning(f"âš ï¸ ä»æœ‰ {remaining_count} ä¸ªæ–‡ä»¶æ®‹ç•™")
            return len(file_paths) - remaining_count

    except Exception as e:
        st.error(f"æ–‡ä»¶å­˜å‚¨æ¸…ç©ºå¤±è´¥: {e}")
        return 0

def get_clickzetta_engine():
    """è·å–ClickZettaå¼•æ“"""
    config = load_env_config()
    return ClickZettaEngine(
        service=config['clickzetta_service'],
        instance=config['clickzetta_instance'],
        workspace=config['clickzetta_workspace'],
        schema=config['clickzetta_schema'],
        username=config['clickzetta_username'],
        password=config['clickzetta_password'],
        vcluster=config['clickzetta_vcluster']
    )

class WebCrawlerDemo:
    """ç½‘ç»œçˆ¬è™«æ¼”ç¤ºç±»"""

    def __init__(self, engine):
        self.engine = engine
        self.setup_storage_services()

    def setup_storage_services(self):
        """åˆå§‹åŒ–æ‰€æœ‰å­˜å‚¨æœåŠ¡"""
        try:
            # æ–‡æ¡£å­˜å‚¨ - å­˜å‚¨ç½‘é¡µå†…å®¹å’Œå…ƒæ•°æ®
            self.doc_store = ClickZettaDocumentStore(
                engine=self.engine,
                table_name="web_crawler_documents"
            )

            # é”®å€¼å­˜å‚¨ - å­˜å‚¨çˆ¬å–çŠ¶æ€å’Œç¼“å­˜
            self.cache_store = ClickZettaStore(
                engine=self.engine,
                table_name="web_crawler_cache"
            )

            # æ–‡ä»¶å­˜å‚¨ - å­˜å‚¨åŸå§‹HTMLæ–‡ä»¶
            self.file_store = ClickZettaFileStore(
                engine=self.engine,
                volume_type="user",
                subdirectory="web_crawler_files"
            )

            # å‘é‡å­˜å‚¨ - è¯­ä¹‰æœç´¢
            if st.session_state.get('dashscope_available'):
                embeddings = DashScopeEmbeddings(
                    dashscope_api_key=os.getenv("DASHSCOPE_API_KEY"),
                    model="text-embedding-v4"
                )
                self.vector_store = ClickZettaVectorStore(
                    engine=self.engine,
                    embeddings=embeddings,
                    table_name="web_crawler_vectors"
                )
            else:
                self.vector_store = None

        except Exception as e:
            st.error(f"å­˜å‚¨æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")

    def crawl_url(self, url: str, progress_callback=None) -> Dict:
        """çˆ¬å–å•ä¸ªURL"""
        result = {
            "success": False,
            "url": url,
            "error": None,
            "content": "",
            "metadata": {},
            "storage_status": {}
        }

        try:
            # éªŒè¯URL
            if not validators.url(url):
                result["error"] = "æ— æ•ˆçš„URLæ ¼å¼"
                return result

            if progress_callback:
                progress_callback("æ­£åœ¨å‘é€è¯·æ±‚...")

            # å‘é€HTTPè¯·æ±‚
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()

            if progress_callback:
                progress_callback("æ­£åœ¨è§£æHTML...")

            # è§£æHTML
            soup = BeautifulSoup(response.content, 'html.parser')

            # æå–å†…å®¹å’Œå…ƒæ•°æ®
            text_content = extract_text_content(response.text)
            metadata = extract_metadata(soup, url)

            # è®¡ç®—URLé”®
            url_key = url_to_key(url)

            if progress_callback:
                progress_callback("æ­£åœ¨å­˜å‚¨åˆ°ClickZetta...")

            # å­˜å‚¨åˆ°å„ç§å­˜å‚¨æœåŠ¡
            storage_status = self.store_crawled_data(
                url_key, url, text_content, metadata, response.text
            )

            result.update({
                "success": True,
                "content": text_content,
                "metadata": metadata,
                "storage_status": storage_status
            })

        except requests.RequestException as e:
            result["error"] = f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}"
        except Exception as e:
            result["error"] = f"çˆ¬å–å¤±è´¥: {e}"

        return result

    def store_crawled_data(self, url_key: str, url: str, content: str,
                          metadata: Dict, raw_html: str) -> Dict:
        """å°†çˆ¬å–çš„æ•°æ®å­˜å‚¨åˆ°å„ç§å­˜å‚¨æœåŠ¡"""
        storage_status = {}

        try:
            # 1. æ–‡æ¡£å­˜å‚¨ - å­˜å‚¨å¤„ç†åçš„å†…å®¹å’Œå…ƒæ•°æ®
            self.doc_store.store_document(
                doc_id=url_key,
                content=content,
                metadata=metadata
            )
            storage_status["document_store"] = "âœ… æˆåŠŸ"

            # 2. é”®å€¼å­˜å‚¨ - å­˜å‚¨çˆ¬å–çŠ¶æ€
            cache_data = [
                (f"crawl_status:{url_key}", b"completed"),
                (f"url_mapping:{url_key}", url.encode()),
                (f"crawl_time:{url_key}", datetime.now().isoformat().encode()),
                (f"content_hash:{url_key}", hashlib.md5(content.encode()).hexdigest().encode())
            ]
            self.cache_store.mset(cache_data)
            storage_status["cache_store"] = "âœ… æˆåŠŸ"

            # 3. æ–‡ä»¶å­˜å‚¨ - å­˜å‚¨åŸå§‹HTML
            self.file_store.store_file(
                file_path=f"{url_key}.html",
                content=raw_html.encode(),
                mime_type="text/html"
            )
            storage_status["file_store"] = "âœ… æˆåŠŸ"

            # 4. å‘é‡å­˜å‚¨ - è¯­ä¹‰æœç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.vector_store and content.strip():
                doc = Document(
                    page_content=content,
                    metadata=metadata
                )
                self.vector_store.add_documents([doc])
                storage_status["vector_store"] = "âœ… æˆåŠŸ"
            else:
                storage_status["vector_store"] = "âš ï¸ è·³è¿‡ (DashScopeä¸å¯ç”¨æˆ–å†…å®¹ä¸ºç©º)"

        except Exception as e:
            storage_status["error"] = f"âŒ å­˜å‚¨å¤±è´¥: {e}"

        return storage_status

    def search_documents(self, query: str, search_type: str = "semantic") -> List[Dict]:
        """æœç´¢å·²çˆ¬å–çš„æ–‡æ¡£"""
        results = []

        try:
            if search_type == "semantic" and self.vector_store:
                # è¯­ä¹‰æœç´¢
                docs = self.vector_store.similarity_search(query, k=5)
                for doc in docs:
                    results.append({
                        "content": doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content,
                        "metadata": doc.metadata,
                        "search_type": "è¯­ä¹‰æœç´¢"
                    })

            elif search_type == "keyword":
                # å…³é”®è¯æœç´¢ (ç®€å•å®ç°)
                # è¿™é‡Œå¯ä»¥ä½¿ç”¨ClickZettaçš„å…¨æ–‡æœç´¢åŠŸèƒ½
                query_result, _ = self.engine.execute_query(f"""
                    SELECT doc_id, doc_content, metadata
                    FROM {self.doc_store.table_name}
                    WHERE doc_content LIKE '%{query}%'
                    LIMIT 5
                """)

                for row in query_result:
                    try:
                        metadata = json.loads(row['metadata']) if row['metadata'] else {}
                    except:
                        metadata = {}

                    results.append({
                        "content": row['doc_content'][:500] + "..." if len(row['doc_content']) > 500 else row['doc_content'],
                        "metadata": metadata,
                        "search_type": "å…³é”®è¯æœç´¢"
                    })

        except Exception as e:
            st.error(f"æœç´¢å¤±è´¥: {e}")

        return results

    def get_storage_stats(self) -> Dict:
        """è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}

        try:
            # æ–‡æ¡£ç»Ÿè®¡
            doc_result, _ = self.engine.execute_query(f"""
                SELECT COUNT(*) as doc_count,
                       AVG(LENGTH(doc_content)) as avg_content_length
                FROM {self.doc_store.table_name}
            """)
            stats["documents"] = doc_result[0] if doc_result else {"doc_count": 0, "avg_content_length": 0}

            # ç¼“å­˜ç»Ÿè®¡
            cache_result, _ = self.engine.execute_query(f"""
                SELECT COUNT(*) as cache_count
                FROM {self.cache_store.table_name}
            """)
            stats["cache"] = cache_result[0] if cache_result else {"cache_count": 0}

            # æ–‡ä»¶ç»Ÿè®¡ (é€šè¿‡ Volume Store API)
            try:
                # list_files() è¿”å› list[tuple[str, int, str]] æ ¼å¼: (æ–‡ä»¶è·¯å¾„, æ–‡ä»¶å¤§å°, å†…å®¹ç±»å‹)
                files = self.file_store.list_files()
                file_count = len(files) if files else 0
                total_size = 0
                if files:
                    for file_info in files:
                        if isinstance(file_info, tuple) and len(file_info) >= 2:
                            # file_info[1] æ˜¯æ–‡ä»¶å¤§å°
                            file_size = file_info[1]
                            total_size += file_size

                stats["files"] = {
                    "file_count": file_count,
                    "total_size": total_size,
                    "total_size_kb": round(total_size / 1024, 2) if total_size > 0 else 0
                }
            except Exception as e:
                stats["files"] = {"file_count": f"è·å–å¤±è´¥: {e}", "total_size": 0}

        except Exception as e:
            st.warning(f"ç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥: {e}")
            stats = {"error": str(e)}

        return stats

def show_help_documentation():
    """æ˜¾ç¤ºå¸®åŠ©æ–‡æ¡£"""
    st.markdown("# ğŸ•·ï¸ ClickZetta ç½‘ç»œçˆ¬è™«ä¸å­˜å‚¨ç³»ç»Ÿ - å­¦ä¹ æŒ‡å—")

    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ¯ åŠŸèƒ½æ¦‚è¿°",
        "ğŸ”§ æŠ€æœ¯æ¶æ„",
        "ğŸ’¡ å®é™…åº”ç”¨",
        "ğŸ“š å­¦ä¹ èµ„æº"
    ])

    with tab1:
        st.markdown("## ğŸ¯ åŠŸèƒ½æ¦‚è¿°")

        st.markdown("""
        ### ğŸŒŸ ä»€ä¹ˆæ˜¯ClickZettaç½‘ç»œçˆ¬è™«ä¸å­˜å‚¨ç³»ç»Ÿï¼Ÿ

        è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç½‘ç»œæ•°æ®é‡‡é›†å’Œå­˜å‚¨è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ClickZettaçš„**å››å¤§å­˜å‚¨æœåŠ¡**æ¥æ„å»ºç°ä»£åŒ–çš„æ•°æ®pipelineï¼š

        **ğŸ”„ æ•°æ®æµç¨‹**ï¼šç½‘é¡µæŠ“å– â†’ å†…å®¹è§£æ â†’ å¤šå­˜å‚¨ååŒ â†’ æ™ºèƒ½æ£€ç´¢
        """)

        # æ ¸å¿ƒåŠŸèƒ½å±•ç¤º
        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            **ğŸ“¥ æ•°æ®é‡‡é›†åŠŸèƒ½**
            - ğŸ•·ï¸ æ™ºèƒ½ç½‘é¡µçˆ¬å–
            - ğŸ“„ HTMLå†…å®¹è§£æ
            - ğŸ·ï¸ å…ƒæ•°æ®æå–
            - ğŸ” å†…å®¹æ¸…ç†ä¼˜åŒ–
            """)

        with col2:
            st.markdown("""
            **ğŸ’¾ å­˜å‚¨ä¸æ£€ç´¢**
            - ğŸ“š ç»“æ„åŒ–æ–‡æ¡£å­˜å‚¨
            - ğŸ—ƒï¸ é«˜é€Ÿé”®å€¼ç¼“å­˜
            - ğŸ“ åŸå§‹æ–‡ä»¶ä¿å­˜
            - ğŸ” AIè¯­ä¹‰æœç´¢
            """)

        st.markdown("""
        ### ğŸ—ï¸ å››å¤§å­˜å‚¨æœåŠ¡ååŒå·¥ä½œ

        æœ¬ç³»ç»Ÿç‹¬ç‰¹åœ°å±•ç¤ºäº†ClickZettaå››ç§å­˜å‚¨æœåŠ¡å¦‚ä½•ååŒå·¥ä½œï¼Œå°±åƒä¸€ä¸ªå®Œæ•´çš„å›¾ä¹¦é¦†ç³»ç»Ÿï¼š
        """)

        storage_services = [
            {
                "name": "ğŸ“š ClickZettaDocumentStore",
                "description": "ç»“æ„åŒ–æ–‡æ¡£åº“",
                "analogy": "å°±åƒå›¾ä¹¦é¦†çš„ä¸»è¦ä¹¦æ¶ï¼Œå­˜å‚¨ä¹¦ç±å†…å®¹å’Œè¯¦ç»†ä¿¡æ¯",
                "data": "ç½‘é¡µæ­£æ–‡ã€æ ‡é¢˜ã€ä½œè€…ã€å‘å¸ƒæ—¶é—´ç­‰ç»“æ„åŒ–ä¿¡æ¯",
                "table": "web_crawler_documentsè¡¨"
            },
            {
                "name": "ğŸ—ƒï¸ ClickZettaStore",
                "description": "é”®å€¼ç¼“å­˜ç³»ç»Ÿ",
                "analogy": "å°±åƒå›¾ä¹¦é¦†çš„ç´¢å¼•å¡ç‰‡ï¼Œå¿«é€ŸæŸ¥æ‰¾ä¹¦ç±çŠ¶æ€å’Œä½ç½®",
                "data": "çˆ¬å–çŠ¶æ€ã€URLæ˜ å°„ã€å†…å®¹å“ˆå¸Œã€æ›´æ–°æ—¶é—´",
                "table": "web_crawler_cacheè¡¨"
            },
            {
                "name": "ğŸ“ ClickZettaFileStore",
                "description": "åŸå§‹æ–‡ä»¶ä»“åº“",
                "analogy": "å°±åƒå›¾ä¹¦é¦†çš„æ¡£æ¡ˆå®¤ï¼Œä¿å­˜åŸå§‹æ–‡æ¡£å’Œæ‰‹ç¨¿",
                "data": "å®Œæ•´HTMLæºç ã€CSSã€JavaScriptç­‰åŸå§‹æ–‡ä»¶",
                "table": "Volumeå­˜å‚¨ï¼ˆæ–‡ä»¶ç³»ç»Ÿï¼‰"
            },
            {
                "name": "ğŸ” ClickZettaVectorStore",
                "description": "AIè¯­ä¹‰æœç´¢",
                "analogy": "å°±åƒå›¾ä¹¦é¦†çš„æ™ºèƒ½æ¨èç³»ç»Ÿï¼Œæ ¹æ®å†…å®¹ç›¸ä¼¼æ€§æ¨èç›¸å…³ä¹¦ç±",
                "data": "æ–‡æœ¬å‘é‡åŒ–è¡¨ç¤ºï¼Œæ”¯æŒè¯­ä¹‰ç›¸ä¼¼æ€§æœç´¢",
                "table": "web_crawler_vectorsè¡¨"
            }
        ]

        for service in storage_services:
            with st.expander(f"{service['name']} - {service['description']}"):
                st.write(f"**ç”Ÿæ´»åŒ–ç†è§£**: {service['analogy']}")
                st.write(f"**å­˜å‚¨å†…å®¹**: {service['data']}")
                st.write(f"**å­˜å‚¨ä½ç½®**: {service['table']}")

        st.success("ğŸ’¡ **æ ¸å¿ƒä¼˜åŠ¿**: å››ç§å­˜å‚¨æœåŠ¡å„å¸å…¶èŒï¼Œæ—¢ä¿è¯äº†æ•°æ®çš„å®Œæ•´æ€§ï¼Œåˆä¼˜åŒ–äº†ä¸åŒåœºæ™¯ä¸‹çš„æŸ¥è¯¢æ€§èƒ½ï¼")

    with tab2:
        st.markdown("## ğŸ”§ æŠ€æœ¯æ¶æ„")

        st.markdown("### ğŸ› ï¸ æ ¸å¿ƒæŠ€æœ¯æ ˆ")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            **ğŸŒ ç½‘ç»œçˆ¬å–æŠ€æœ¯**
            - `requests`: HTTPè¯·æ±‚å¤„ç†
            - `BeautifulSoup`: HTMLè§£æ
            - `html2text`: å†…å®¹æ¸…ç†
            - `validators`: URLéªŒè¯
            """)

        with col2:
            st.markdown("""
            **ğŸ¤– AIä¸å­˜å‚¨æŠ€æœ¯**
            - `ClickZetta`: ç»Ÿä¸€æ•°æ®å­˜å‚¨å¹³å°
            - `LangChain`: AIåº”ç”¨æ¡†æ¶
            - `DashScope`: é˜¿é‡Œäº‘å‘é‡åŒ–æœåŠ¡
            - `Streamlit`: å¯è§†åŒ–ç•Œé¢
            """)

        st.markdown("### ğŸ—ï¸ ç³»ç»Ÿæ¶æ„å›¾")

        st.markdown("""
        ```
        ğŸŒ ç½‘é¡µè¾“å…¥
            â†“
        ğŸ•·ï¸ ç½‘ç»œçˆ¬è™« (requests + BeautifulSoup)
            â†“
        ğŸ“„ å†…å®¹è§£æ (html2text + metadata extraction)
            â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           ClickZettaå­˜å‚¨å±‚               â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ ğŸ“š DocumentStore â”‚ ğŸ—ƒï¸ Store (Cache)     â”‚
        â”‚ (ç»“æ„åŒ–æ–‡æ¡£)      â”‚ (çŠ¶æ€ä¸æ˜ å°„)          â”‚
        â”‚                  â”‚                      â”‚
        â”‚ ğŸ“ FileStore     â”‚ ğŸ” VectorStore       â”‚
        â”‚ (åŸå§‹æ–‡ä»¶)        â”‚ (è¯­ä¹‰æœç´¢)            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
        ğŸ” æ™ºèƒ½æ£€ç´¢ (å…³é”®è¯ + è¯­ä¹‰æœç´¢)
            â†“
        ğŸ“Š å¯è§†åŒ–å±•ç¤º (Streamlit)
        ```
        """)

        st.markdown("### ğŸ”„ æ•°æ®å¤„ç†æµç¨‹")

        process_steps = [
            {
                "step": "1ï¸âƒ£ ç½‘é¡µæŠ“å–",
                "description": "ä½¿ç”¨requestsè·å–HTMLå†…å®¹ï¼ŒBeautifulSoupè§£æDOMç»“æ„",
                "code": """
# å‘é€HTTPè¯·æ±‚
response = requests.get(url, headers=headers, timeout=30)

# è§£æHTMLå†…å®¹
soup = BeautifulSoup(response.content, 'html.parser')
"""
            },
            {
                "step": "2ï¸âƒ£ å†…å®¹æå–",
                "description": "æå–æ­£æ–‡å†…å®¹å’Œå…ƒæ•°æ®ä¿¡æ¯",
                "code": """
# æå–æ­£æ–‡
text_content = extract_text_content(response.text)

# æå–å…ƒæ•°æ®
metadata = extract_metadata(soup, url)
"""
            },
            {
                "step": "3ï¸âƒ£ å¤šå­˜å‚¨å†™å…¥",
                "description": "åŒæ—¶å†™å…¥å››ç§å­˜å‚¨æœåŠ¡ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§",
                "code": """
# æ–‡æ¡£å­˜å‚¨
doc_store.store_document(url_key, content, metadata)

# é”®å€¼å­˜å‚¨
cache_store.mset([(status_key, status), (url_key, url)])

# æ–‡ä»¶å­˜å‚¨
file_store.store_file(f"{url_key}.html", raw_html)

# å‘é‡å­˜å‚¨
vector_store.add_documents([Document(content, metadata)])
"""
            },
            {
                "step": "4ï¸âƒ£ æ™ºèƒ½æ£€ç´¢",
                "description": "æ”¯æŒå…³é”®è¯å’Œè¯­ä¹‰ä¸¤ç§æœç´¢æ–¹å¼",
                "code": """
# è¯­ä¹‰æœç´¢
docs = vector_store.similarity_search(query, k=5)

# å…³é”®è¯æœç´¢
results = engine.execute_query(
    f"SELECT * FROM documents WHERE content LIKE '%{query}%'"
)
"""
            }
        ]

        for step_info in process_steps:
            with st.expander(step_info["step"]):
                st.write(step_info["description"])
                st.code(step_info["code"], language="python")

        st.markdown("### ğŸ” è¡¨ç»“æ„è¯¦æƒ…")

        if st.button("ğŸ—„ï¸ æŸ¥çœ‹å­˜å‚¨è¡¨ç»“æ„", key="crawler_table_structure"):
            st.markdown("""
            **ğŸ“š web_crawler_documentsè¡¨ (DocumentStore)**
            ```sql
            - doc_id: VARCHAR      # URLçš„MD5å“ˆå¸Œå€¼
            - doc_content: TEXT    # æå–çš„ç½‘é¡µæ­£æ–‡
            - metadata: JSON       # æ ‡é¢˜ã€ä½œè€…ã€å‘å¸ƒæ—¶é—´ç­‰å…ƒæ•°æ®
            - created_at: TIMESTAMP
            ```

            **ğŸ—ƒï¸ web_crawler_cacheè¡¨ (Store)**
            ```sql
            - key: VARCHAR         # ç¼“å­˜é”® (å¦‚: crawl_status:xxxxx)
            - value: BYTEA         # ç¼“å­˜å€¼ (çŠ¶æ€ã€URLæ˜ å°„ç­‰)
            - created_at: TIMESTAMP
            ```

            **ğŸ“ FileStore (Volumeå­˜å‚¨)**
            ```
            - æ–‡ä»¶è·¯å¾„: web_crawler_files/{url_hash}.html
            - å†…å®¹ç±»å‹: text/html
            - æ–‡ä»¶å¤§å°: è‡ªåŠ¨è®¡ç®—
            ```

            **ğŸ” web_crawler_vectorsè¡¨ (VectorStore)**
            ```sql
            - id: VARCHAR          # æ–‡æ¡£ID
            - embedding: FLOAT[]   # 1536ç»´å‘é‡ (text-embedding-v4)
            - document: TEXT       # åŸå§‹æ–‡æ¡£å†…å®¹
            - metadata: JSON       # æ–‡æ¡£å…ƒæ•°æ®
            ```
            """)

    with tab3:
        st.markdown("## ğŸ’¡ å®é™…åº”ç”¨åœºæ™¯")

        st.markdown("### ğŸŒŸ ä¼ä¸šçº§åº”ç”¨åœºæ™¯")

        use_cases = [
            {
                "title": "ğŸ“° åª’ä½“å†…å®¹èšåˆ",
                "description": "æ–°é—»æœºæ„å¯ä»¥ä½¿ç”¨æ­¤ç³»ç»Ÿç›‘æ§å¤šä¸ªæ–°é—»æºï¼Œè‡ªåŠ¨é‡‡é›†ã€åˆ†æå’Œåˆ†å‘æ–°é—»å†…å®¹",
                "benefits": ["å®æ—¶æ–°é—»æ›´æ–°", "é‡å¤å†…å®¹å»é‡", "æ™ºèƒ½å†…å®¹æ¨è", "å†å²æ•°æ®æŸ¥è¯¢"],
                "example": "æ¯æ—¥è‡ªåŠ¨çˆ¬å–100+æ–°é—»ç½‘ç«™ï¼Œæ™ºèƒ½åˆ†ç±»å’Œæ¨èç›¸å…³æ–°é—»"
            },
            {
                "title": "ğŸ¢ ä¼ä¸šçŸ¥è¯†ç®¡ç†",
                "description": "ä¼ä¸šå¯ä»¥çˆ¬å–å†…å¤–éƒ¨æ–‡æ¡£ã€æ”¿ç­–ã€æµç¨‹ï¼Œæ„å»ºæ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ",
                "benefits": ["æ–‡æ¡£è‡ªåŠ¨æ”¶é›†", "æ™ºèƒ½æœç´¢å¼•æ“", "ç‰ˆæœ¬å˜æ›´è¿½è¸ª", "çŸ¥è¯†å›¾è°±æ„å»º"],
                "example": "çˆ¬å–å…¬å¸å†…ç½‘æ–‡æ¡£ï¼Œå‘˜å·¥å¯é€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç›¸å…³æ”¿ç­–"
            },
            {
                "title": "ğŸ›ï¸ ç”µå•†ä»·æ ¼ç›‘æ§",
                "description": "ç”µå•†å¹³å°å¯ä»¥ç›‘æ§ç«å“ä»·æ ¼ã€åº“å­˜ã€è¯„ä»·ç­‰ä¿¡æ¯ï¼Œåˆ¶å®šåŠ¨æ€å®šä»·ç­–ç•¥",
                "benefits": ["å®æ—¶ä»·æ ¼è¿½è¸ª", "åº“å­˜çŠ¶æ€ç›‘æ§", "ç”¨æˆ·è¯„ä»·åˆ†æ", "å¸‚åœºè¶‹åŠ¿é¢„æµ‹"],
                "example": "ç›‘æ§1000+ç«å“å•†å“ï¼Œè‡ªåŠ¨è°ƒæ•´ä»·æ ¼ç­–ç•¥"
            },
            {
                "title": "ğŸ”¬ å­¦æœ¯ç ”ç©¶åŠ©æ‰‹",
                "description": "ç ”ç©¶äººå‘˜å¯ä»¥è‡ªåŠ¨æ”¶é›†è®ºæ–‡ã€æŠ¥å‘Šï¼Œè¿›è¡Œæ–‡çŒ®ç»¼è¿°å’Œç ”ç©¶è¶‹åŠ¿åˆ†æ",
                "benefits": ["è®ºæ–‡è‡ªåŠ¨æ”¶é›†", "ç ”ç©¶è¶‹åŠ¿åˆ†æ", "å¼•ç”¨å…³ç³»æŒ–æ˜", "é‡å¤ç ”ç©¶é¿å…"],
                "example": "çˆ¬å–arXivã€IEEEç­‰å¹³å°ï¼Œä¸ºAIç ”ç©¶æä¾›æ–‡çŒ®æ”¯æŒ"
            },
            {
                "title": "ğŸ›ï¸ æ”¿åºœå…¬å¼€ä¿¡æ¯ç›‘æ§",
                "description": "ç›‘æ§æ”¿åºœç½‘ç«™çš„æ”¿ç­–æ›´æ–°ã€å…¬å‘Šå‘å¸ƒï¼ŒåŠæ—¶å“åº”æ”¿ç­–å˜åŒ–",
                "benefits": ["æ”¿ç­–å®æ—¶ç›‘æ§", "æ³•è§„å˜æ›´æé†’", "å…¬å¼€ä¿¡æ¯å½’æ¡£", "å½±å“è¯„ä¼°åˆ†æ"],
                "example": "ç›‘æ§æ”¿åºœå®˜ç½‘ï¼Œè‡ªåŠ¨æå–æ”¿ç­–æ–‡ä»¶å¹¶åˆ†æå¯¹ä¼ä¸šçš„å½±å“"
            }
        ]

        for i, case in enumerate(use_cases):
            with st.expander(f"{case['title']}", expanded=i==0):
                st.write(f"**åº”ç”¨æè¿°**: {case['description']}")
                st.write(f"**å®é™…æ¡ˆä¾‹**: {case['example']}")

                col1, col2 = st.columns(2)
                with col1:
                    st.write("**æ ¸å¿ƒä¼˜åŠ¿**:")
                    for benefit in case['benefits']:
                        st.write(f"- {benefit}")

                with col2:
                    if i == 0:  # åª’ä½“å†…å®¹
                        st.code("""
# æ–°é—»çˆ¬å–ç¤ºä¾‹
urls = [
    "https://news.sina.com.cn",
    "https://news.163.com",
    "https://www.thepaper.cn"
]

for url in urls:
    result = crawler.crawl_url(url)
    # è‡ªåŠ¨åˆ†ç±»å’Œå»é‡
    classify_news(result)
""", language="python")
                    elif i == 1:  # çŸ¥è¯†ç®¡ç†
                        st.code("""
# çŸ¥è¯†åº“æ„å»º
knowledge_sources = [
    "å†…éƒ¨æ–‡æ¡£ç³»ç»Ÿ",
    "æ”¿ç­–åˆ¶åº¦ç½‘ç«™",
    "è¡Œä¸šæ ‡å‡†æ–‡æ¡£"
]

# æ™ºèƒ½é—®ç­”
query = "å…¬å¸å·®æ—…æŠ¥é”€æ”¿ç­–"
results = search_documents(query, "semantic")
""", language="python")
                    elif i == 2:  # ç”µå•†ç›‘æ§
                        st.code("""
# ä»·æ ¼ç›‘æ§
products = ["iPhone 15", "åä¸ºMate60"]

for product in products:
    price_data = crawl_ecommerce_data(product)
    if price_changed(price_data):
        update_pricing_strategy(product)
""", language="python")
                    elif i == 3:  # å­¦æœ¯ç ”ç©¶
                        st.code("""
# è®ºæ–‡æ”¶é›†
keywords = ["machine learning", "deep learning"]

papers = crawl_academic_papers(keywords)
trends = analyze_research_trends(papers)
generate_literature_review(trends)
""", language="python")
                    else:  # æ”¿åºœç›‘æ§
                        st.code("""
# æ”¿ç­–ç›‘æ§
gov_sites = [
    "å·¥ä¿¡éƒ¨å®˜ç½‘",
    "å‘æ”¹å§”ç½‘ç«™",
    "å¤®è¡Œå®˜ç½‘"
]

policies = monitor_policy_changes(gov_sites)
analyze_business_impact(policies)
""", language="python")

        st.markdown("### âš¡ æ€§èƒ½ä¼˜åŠ¿")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            **ğŸš€ ClickZettaå­˜å‚¨ä¼˜åŠ¿**
            - **10å€æ€§èƒ½æå‡**: äº‘åŸç”Ÿæ¶æ„
            - **æ— ç¼æ‰©å±•**: GBåˆ°PBçº§æ•°æ®
            - **ACIDäº‹åŠ¡**: ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
            - **SQLå…¼å®¹**: ç†Ÿæ‚‰çš„æŸ¥è¯¢è¯­æ³•
            """)

        with col2:
            st.markdown("""
            **ğŸ¤– AIèƒ½åŠ›é›†æˆ**
            - **è¯­ä¹‰æœç´¢**: ç†è§£å†…å®¹å«ä¹‰
            - **æ™ºèƒ½æ¨è**: åŸºäºç›¸ä¼¼æ€§åŒ¹é…
            - **è‡ªåŠ¨åˆ†ç±»**: å†…å®¹æ™ºèƒ½æ ‡ç­¾
            - **è¶‹åŠ¿åˆ†æ**: æ•°æ®æ´å¯ŸæŒ–æ˜
            """)

    with tab4:
        st.markdown("## ğŸ“š å­¦ä¹ èµ„æº")

        st.markdown("### ğŸ“– å®˜æ–¹æ–‡æ¡£")

        doc_links = [
            {
                "title": "ClickZetta å®˜æ–¹æ–‡æ¡£",
                "url": "https://www.yunqi.tech/documents/",
                "description": "å®Œæ•´çš„ClickZettaå¹³å°ä½¿ç”¨æŒ‡å—"
            },
            {
                "title": "LangChain æ–‡æ¡£",
                "url": "https://python.langchain.com/",
                "description": "LangChainæ¡†æ¶å®Œæ•´æ–‡æ¡£"
            },
            {
                "title": "DashScope APIæ–‡æ¡£",
                "url": "https://help.aliyun.com/zh/dashscope/",
                "description": "é˜¿é‡Œäº‘å¤§æ¨¡å‹æœåŠ¡APIæ–‡æ¡£"
            },
            {
                "title": "Streamlit æ–‡æ¡£",
                "url": "https://docs.streamlit.io/",
                "description": "Streamlitåº”ç”¨å¼€å‘æ–‡æ¡£"
            }
        ]

        for doc in doc_links:
            st.markdown(f"- **[{doc['title']}]({doc['url']})**: {doc['description']}")

        st.markdown("### ğŸ› ï¸ å¿«é€Ÿå¼€å§‹")

        st.markdown("""
        #### 1ï¸âƒ£ ç¯å¢ƒå‡†å¤‡
        ```bash
        # å®‰è£…ä¾èµ–
        pip install -r requirements.txt

        # é…ç½®ç¯å¢ƒå˜é‡
        cp .env.example .env
        # ç¼–è¾‘.envæ–‡ä»¶ï¼Œå¡«å…¥ClickZettaå’ŒDashScopeé…ç½®
        ```

        #### 2ï¸âƒ£ è¿è¡Œåº”ç”¨
        ```bash
        # å¯åŠ¨åº”ç”¨
        streamlit run streamlit_app.py

        # æ‰“å¼€æµè§ˆå™¨è®¿é—® http://localhost:8501
        ```

        #### 3ï¸âƒ£ å¼€å§‹çˆ¬å–
        1. åœ¨"ğŸ•·ï¸ ç½‘é¡µçˆ¬å–"æ ‡ç­¾é¡µè¾“å…¥URL
        2. ç‚¹å‡»"ğŸš€ å¼€å§‹çˆ¬å–"æŒ‰é’®
        3. åœ¨"ğŸ” å†…å®¹æœç´¢"æ ‡ç­¾é¡µæµ‹è¯•æœç´¢åŠŸèƒ½
        4. åœ¨"ğŸ“Š å­˜å‚¨ç»Ÿè®¡"æ ‡ç­¾é¡µæŸ¥çœ‹æ•°æ®çŠ¶æ€

        #### 4ï¸âƒ£ æ–‡ä»¶ç®¡ç†æ“ä½œ â­
        **é‡è¦æç¤ºï¼šå¯é çš„æ–‡ä»¶åˆ é™¤æ–¹æ³•**

        å½“éœ€è¦æ¸…ç©ºå­˜å‚¨æ•°æ®æ—¶ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹æ“ä½œï¼š
        1. ç‚¹å‡»ä¾§è¾¹æ çš„"ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ•°æ®"æŒ‰é’®
        2. ç³»ç»Ÿä¼šè‡ªåŠ¨ä½¿ç”¨**ä¼˜åŒ–çš„åˆ é™¤ç­–ç•¥**ï¼š
           - è¯†åˆ«æ‰€æœ‰ç›¸å…³æ–‡ä»¶é”®ï¼ˆä¸»é”® + å…ƒæ•°æ®é”®ï¼‰
           - ä¸€æ¬¡æ€§æ‰¹é‡åˆ é™¤æ‰€æœ‰ç›¸å…³æ•°æ®
           - ç¡®ä¿æ–‡ä»¶å®Œå…¨ä»å­˜å‚¨ä¸­ç§»é™¤

        **æŠ€æœ¯è¯´æ˜**ï¼š
        - ğŸ”‘ **å…³é”®å‘ç°**: æ¯ä¸ªæ–‡ä»¶æœ‰ä¸¤ä¸ªé”®ï¼ˆä¸»é”® + `_metadata_`é”®ï¼‰ï¼Œå¿…é¡»åŒæ—¶åˆ é™¤
        - âœ… **æœ‰æ•ˆæ–¹æ³•**: ç³»ç»Ÿå·²ä¼˜åŒ–ä¸ºç›´æ¥ä½¿ç”¨ `volume_store.mdelete()`
        - ğŸ¯ **åˆ é™¤ç­–ç•¥**: è‡ªåŠ¨è¯†åˆ«å¹¶åˆ é™¤æ‰€æœ‰ç›¸å…³é”®ï¼Œç¡®ä¿å®Œå…¨æ¸…ç†

        ```python
        # ç³»ç»Ÿå®ç°çš„åˆ é™¤æ–¹å¼
        keys_to_delete = []
        for file_path in file_paths:
            keys_to_delete.append(file_path)              # ä¸»æ–‡ä»¶é”®
            keys_to_delete.append(f"_metadata_{file_path}") # å…ƒæ•°æ®é”®

        # ä¸€æ¬¡æ€§åˆ é™¤æ‰€æœ‰ç›¸å…³é”®
        crawler.file_store.volume_store.mdelete(keys_to_delete)
        ```
        """)

        st.markdown("### ğŸ”§ è‡ªå®šä¹‰å¼€å‘")

        st.markdown("""
        #### æ‰©å±•çˆ¬è™«åŠŸèƒ½
        ```python
        # è‡ªå®šä¹‰çˆ¬è™«ç±»
        class CustomWebCrawler(WebCrawlerDemo):
            def custom_parse_content(self, soup):
                # æ·»åŠ è‡ªå®šä¹‰è§£æé€»è¾‘
                pass

            def custom_metadata_extraction(self, soup):
                # æ·»åŠ è‡ªå®šä¹‰å…ƒæ•°æ®æå–
                pass
        ```

        #### æ‰©å±•å­˜å‚¨åŠŸèƒ½
        ```python
        # æ·»åŠ æ–°çš„å­˜å‚¨æœåŠ¡
        custom_store = ClickZettaCustomStore(
            engine=engine,
            table_name="custom_table"
        )

        # è‡ªå®šä¹‰æ•°æ®å¤„ç†æµç¨‹
        def custom_data_pipeline(data):
            # æ·»åŠ æ•°æ®é¢„å¤„ç†é€»è¾‘
            processed_data = preprocess(data)

            # å­˜å‚¨åˆ°è‡ªå®šä¹‰è¡¨
            custom_store.store(processed_data)
        ```
        """)

        st.markdown("### ğŸ’¡ æœ€ä½³å®è·µ")

        best_practices = [
            {
                "category": "ğŸ•·ï¸ çˆ¬è™«ä¼˜åŒ–",
                "tips": [
                    "è®¾ç½®åˆç†çš„è¯·æ±‚é—´éš”ï¼Œé¿å…è¢«åçˆ¬è™«ç³»ç»Ÿæ‹¦æˆª",
                    "ä½¿ç”¨ä»£ç†æ± å’ŒUser-Agentè½®æ¢æé«˜æˆåŠŸç‡",
                    "é’ˆå¯¹ä¸åŒç½‘ç«™è®¾è®¡ä¸“é—¨çš„è§£æç­–ç•¥",
                    "å®æ–½æ–­ç‚¹ç»­çˆ¬åŠŸèƒ½ï¼Œæé«˜å¤§æ‰¹é‡ä»»åŠ¡çš„ç¨³å®šæ€§"
                ]
            },
            {
                "category": "ğŸ’¾ å­˜å‚¨ç­–ç•¥",
                "tips": [
                    "æ ¹æ®æŸ¥è¯¢æ¨¡å¼é€‰æ‹©åˆé€‚çš„å­˜å‚¨æœåŠ¡ç»„åˆ",
                    "è®¾è®¡åˆç†çš„æ•°æ®åˆ†åŒºç­–ç•¥ï¼Œæé«˜æŸ¥è¯¢æ€§èƒ½",
                    "å®šæœŸæ¸…ç†è¿‡æœŸæ•°æ®ï¼Œæ§åˆ¶å­˜å‚¨æˆæœ¬",
                    "å»ºç«‹æ•°æ®å¤‡ä»½å’Œæ¢å¤æœºåˆ¶"
                ]
            },
            {
                "category": "ğŸ” æœç´¢ä¼˜åŒ–",
                "tips": [
                    "ä¸ºä¸åŒç±»å‹å†…å®¹è®¾è®¡ä¸“é—¨çš„å‘é‡åŒ–ç­–ç•¥",
                    "ç»“åˆå…³é”®è¯å’Œè¯­ä¹‰æœç´¢ï¼Œæä¾›æ›´å…¨é¢çš„ç»“æœ",
                    "å»ºç«‹æœç´¢ç»“æœç›¸å…³æ€§è¯„åˆ†æœºåˆ¶",
                    "å®æ–½æœç´¢æ—¥å¿—åˆ†æï¼ŒæŒç»­ä¼˜åŒ–æœç´¢è´¨é‡"
                ]
            },
            {
                "category": "ğŸš€ æ€§èƒ½ä¼˜åŒ–",
                "tips": [
                    "ä½¿ç”¨å¼‚æ­¥å¹¶å‘çˆ¬å–ï¼Œæé«˜æ•°æ®é‡‡é›†æ•ˆç‡",
                    "å®æ–½æ™ºèƒ½ç¼“å­˜ç­–ç•¥ï¼Œå‡å°‘é‡å¤è®¡ç®—",
                    "ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢ï¼Œä½¿ç”¨ç´¢å¼•å’ŒæŸ¥è¯¢ä¼˜åŒ–",
                    "ç›‘æ§ç³»ç»Ÿæ€§èƒ½ï¼ŒåŠæ—¶å‘ç°å’Œè§£å†³ç“¶é¢ˆ"
                ]
            }
        ]

        for practice in best_practices:
            with st.expander(practice["category"]):
                for tip in practice["tips"]:
                    st.write(f"- {tip}")

        st.markdown("### ğŸ¤ ç¤¾åŒºæ”¯æŒ")

        st.markdown("""
        - **æŠ€æœ¯äº¤æµ**: åŠ å…¥ClickZettaæŠ€æœ¯äº¤æµç¾¤
        - **é—®é¢˜åé¦ˆ**: é€šè¿‡GitHub IssuesæŠ¥å‘Šé—®é¢˜
        - **åŠŸèƒ½å»ºè®®**: æäº¤åŠŸèƒ½éœ€æ±‚å’Œæ”¹è¿›å»ºè®®
        - **æ¡ˆä¾‹åˆ†äº«**: åˆ†äº«ä½ çš„åº”ç”¨æ¡ˆä¾‹å’Œæœ€ä½³å®è·µ
        """)

        st.success("ğŸ‰ æ­å–œï¼ä½ å·²ç»æŒæ¡äº†ClickZettaç½‘ç»œçˆ¬è™«ä¸å­˜å‚¨ç³»ç»Ÿçš„æ ¸å¿ƒçŸ¥è¯†ã€‚ç°åœ¨å°±å¼€å§‹æ„å»ºä½ è‡ªå·±çš„æ™ºèƒ½æ•°æ®é‡‡é›†ç³»ç»Ÿå§ï¼")

def main():
    """ä¸»å‡½æ•°"""
    # é¡µé¢å¯¼èˆª
    page_selection = st.selectbox(
        "é€‰æ‹©åŠŸèƒ½é¡µé¢",
        ["ğŸ•·ï¸ ç½‘ç»œçˆ¬è™«", "ğŸ“š å­¦ä¹ æŒ‡å—"],
        key="crawler_page_selection"
    )

    if page_selection == "ğŸ“š å­¦ä¹ æŒ‡å—":
        show_help_documentation()
        return

    # åŸæœ‰çš„ä¸»è¦åŠŸèƒ½ç•Œé¢
    st.title("ğŸ•·ï¸ ClickZetta Web Crawler & Storage Demo")
    st.markdown("### å±•ç¤ºLangChainç½‘ç«™çˆ¬å–ä¸ClickZettaå­˜å‚¨æœåŠ¡çš„å®Œæ•´é›†æˆ")

    # åˆå§‹åŒ–session state
    if 'crawl_results' not in st.session_state:
        st.session_state.crawl_results = []
    if 'show_content' not in st.session_state:
        st.session_state.show_content = {}

    # æ£€æŸ¥ä¾èµ–
    if not DEPENDENCIES_AVAILABLE:
        st.error(f"ç¼ºå°‘å¿…è¦ä¾èµ–: {import_error}")
        st.info("è¯·è¿è¡Œ: pip install -r requirements.txt")
        return

    # æ£€æŸ¥ç¯å¢ƒé…ç½®
    env_status = check_environment()
    display_environment_status(env_status)

    if not env_status["clickzetta_available"]:
        st.error("ClickZettaé…ç½®ä¸å®Œæ•´ï¼Œæ— æ³•ä½¿ç”¨å­˜å‚¨æœåŠ¡")
        return

    # åˆ›å»ºä¾§è¾¹æ ä¿¡æ¯
    create_sidebar_info()

    # åˆå§‹åŒ–ClickZettaå¼•æ“
    try:
        engine = get_clickzetta_engine()
        crawler = WebCrawlerDemo(engine)
        st.success("âœ… ClickZettaå­˜å‚¨æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")

        # å¤„ç†æ•°æ®æ¸…ç©ºè¯·æ±‚
        if st.session_state.get('clear_data_requested', False):
            st.info("æ­£åœ¨æ¸…ç©ºæ•°æ®...")

            try:
                # æ¸…ç©ºå„ç§å­˜å‚¨è¡¨
                tables_to_clear = [
                    "web_crawler_documents",
                    "web_crawler_cache",
                    "web_crawler_vectors"
                ]

                for table in tables_to_clear:
                    try:
                        delete_query = f"DELETE FROM {table}"
                        engine.execute_query(delete_query)
                    except:
                        # è¡¨ä¸å­˜åœ¨æ˜¯æ­£å¸¸çš„
                        pass

                # æ¸…ç©ºæ–‡ä»¶å­˜å‚¨ï¼ˆä½¿ç”¨crawlerå®ä¾‹ï¼‰
                deleted_count = clear_file_storage(crawler)

                # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„è¢«åˆ é™¤
                verification_files = crawler.file_store.list_files()
                remaining_files = len(verification_files) if verification_files else 0

                if remaining_files > 0:
                    st.warning(f"âš ï¸ æ³¨æ„ï¼šåˆ é™¤äº† {deleted_count} ä¸ªæ–‡ä»¶ï¼Œä½†ä»æœ‰ {remaining_files} ä¸ªæ–‡ä»¶æ®‹ç•™")

                    # åˆ†ææ®‹ç•™æ–‡ä»¶æ˜¯å¦ä¸åˆ é™¤çš„æ–‡ä»¶ç›¸åŒ
                    remaining_paths = set()
                    if verification_files:
                        for file_info in verification_files:
                            if isinstance(file_info, tuple) and len(file_info) >= 1:
                                remaining_paths.add(file_info[0])

                    with st.expander(f"æŸ¥çœ‹æ®‹ç•™æ–‡ä»¶è¯¦æƒ… ({len(verification_files)}ä¸ª)"):
                        for file_info in verification_files:
                            if isinstance(file_info, tuple) and len(file_info) >= 1:
                                st.write(f"â€¢ {file_info[0]}")

                    # å°è¯•å†æ¬¡åˆ é™¤æ®‹ç•™æ–‡ä»¶
                    col1, col2 = st.columns(2)
                    with col1:
                        if st.button("ğŸ”„ å°è¯•å†æ¬¡åˆ é™¤æ®‹ç•™æ–‡ä»¶"):
                            try:
                                remaining_file_paths = [f[0] for f in verification_files if isinstance(f, tuple) and len(f) >= 1]
                                if remaining_file_paths:
                                    st.info(f"å°è¯•åˆ é™¤ {len(remaining_file_paths)} ä¸ªæ®‹ç•™æ–‡ä»¶...")
                                    # åŒ…å«ä¸»é”®å’Œå…ƒæ•°æ®é”®
                                    all_remaining_keys = []
                                    for key in remaining_file_paths:
                                        all_remaining_keys.append(key)
                                        all_remaining_keys.append(f"_metadata_{key}")
                                    try:
                                        crawler.file_store.mdelete(all_remaining_keys)
                                    except Exception as fs_err:
                                        st.warning(f"FileStoreåˆ é™¤å¤±è´¥ï¼Œå°è¯•åº•å±‚åˆ é™¤: {fs_err}")
                                        crawler.file_store.volume_store.mdelete(all_remaining_keys)

                                    # å†æ¬¡éªŒè¯
                                    final_check = crawler.file_store.list_files()
                                    final_count = len(final_check) if final_check else 0

                                    if final_count == 0:
                                        st.success("âœ… æ®‹ç•™æ–‡ä»¶åˆ é™¤æˆåŠŸ!")
                                    else:
                                        st.error(f"âŒ ä»æœ‰ {final_count} ä¸ªæ–‡ä»¶æ— æ³•åˆ é™¤ï¼Œå¯èƒ½å­˜åœ¨æƒé™é—®é¢˜æˆ–APIé™åˆ¶")
                            except Exception as retry_e:
                                st.error(f"âŒ å†æ¬¡åˆ é™¤å¤±è´¥: {retry_e}")

                else:
                    st.success("âœ… æ–‡ä»¶å­˜å‚¨å·²å®Œå…¨æ¸…ç©º")

                # é‡ç½®sessionçŠ¶æ€
                if 'crawl_results' in st.session_state:
                    st.session_state.crawl_results = []

                # æ¸…é™¤æ¸…ç©ºè¯·æ±‚æ ‡å¿—
                st.session_state.clear_data_requested = False

                # è®¾ç½®æ ‡è®°ä»¥ä¾¿ä¸‹æ¬¡è®¿é—®ç»Ÿè®¡é¡µé¢æ—¶è‡ªåŠ¨åˆ·æ–°
                st.session_state.force_stats_refresh = True

                st.success("âœ… æ‰€æœ‰æ•°æ®å·²æ¸…ç©ºï¼Œè¯·é‡æ–°å¼€å§‹çˆ¬å–")
                st.rerun()

            except Exception as e:
                st.error(f"âŒ æ¸…ç©ºæ•°æ®å¤±è´¥: {e}")
                st.session_state.clear_data_requested = False

        # è‡ªåŠ¨æ£€æµ‹å¹¶æ˜¾ç¤ºå·²æœ‰æ•°æ®ç»Ÿè®¡
        if st.session_state.get('dashscope_available', False):
            try:
                # æ£€æŸ¥å„ä¸ªå­˜å‚¨æœåŠ¡çš„æ•°æ®é‡
                storage_stats = {}

                # æ£€æŸ¥DocumentStoreæ•°æ®
                try:
                    doc_count_query = f"SELECT COUNT(*) as count FROM {crawler.doc_store.table_name}"
                    doc_result, _ = engine.execute_query(doc_count_query)
                    storage_stats['documents'] = doc_result[0]['count'] if doc_result else 0
                except:
                    storage_stats['documents'] = 0

                # æ£€æŸ¥Cacheæ•°æ®
                try:
                    cache_count_query = f"SELECT COUNT(*) as count FROM {crawler.cache_store.table_name}"
                    cache_result, _ = engine.execute_query(cache_count_query)
                    storage_stats['cache'] = cache_result[0]['count'] if cache_result else 0
                except:
                    storage_stats['cache'] = 0

                # æ£€æŸ¥VectorStoreæ•°æ®
                try:
                    if crawler.vector_store:
                        vector_count_query = f"SELECT COUNT(*) as count FROM {crawler.vector_store.table_name}"
                        vector_result, _ = engine.execute_query(vector_count_query)
                        storage_stats['vectors'] = vector_result[0]['count'] if vector_result else 0
                    else:
                        storage_stats['vectors'] = 0
                except:
                    storage_stats['vectors'] = 0

                # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
                total_data = sum(storage_stats.values())
                if total_data > 0:
                    st.info(f"ğŸ‰ æ£€æµ‹åˆ°å·²å­˜åœ¨æ•°æ®: ğŸ“šæ–‡æ¡£{storage_stats['documents']}æ¡ | ğŸ—‚ï¸ç¼“å­˜{storage_stats['cache']}æ¡ | ğŸ§ å‘é‡{storage_stats['vectors']}æ¡ï¼Œå¯ç›´æ¥ä½¿ç”¨æœç´¢åŠŸèƒ½")
            except Exception as e:
                # æ•°æ®æ£€æµ‹å¤±è´¥ï¼Œä¸å½±å“ä¸»è¦åŠŸèƒ½
                pass

    except Exception as e:
        st.error(f"âŒ ClickZettaè¿æ¥å¤±è´¥: {e}")
        return

    # ä¸»ç•Œé¢æ ‡ç­¾é¡µ
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ•·ï¸ ç½‘é¡µçˆ¬å–", "ğŸ” å†…å®¹æœç´¢", "ğŸ“Š å­˜å‚¨ç»Ÿè®¡", "ğŸ’¡ åŠŸèƒ½æ¼”ç¤º"])

    with tab1:
        st.header("ç½‘é¡µçˆ¬å–ä¸å­˜å‚¨")

        # URLè¾“å…¥
        url_input = st.text_input(
            "è¯·è¾“å…¥è¦çˆ¬å–çš„ç½‘é¡µURL:",
            value="https://www.yunqi.tech",
            placeholder="https://www.yunqi.tech",
            help="æ”¯æŒå¤§å¤šæ•°ç½‘ç«™ï¼Œå»ºè®®ä½¿ç”¨æ–°é—»ã€åšå®¢ç­‰å†…å®¹ä¸°å¯Œçš„é¡µé¢"
        )

        # æ‰¹é‡URLè¾“å…¥
        with st.expander("æ‰¹é‡çˆ¬å– (æ¯è¡Œä¸€ä¸ªURL)"):
            batch_urls = st.text_area(
                "æ‰¹é‡URLåˆ—è¡¨:",
                placeholder="https://www.yunqi.tech\nhttps://www.yunqi.tech/products\nhttps://www.yunqi.tech/solutions",
                height=100
            )

        col1, col2 = st.columns(2)

        with col1:
            if st.button("ğŸš€ å¼€å§‹çˆ¬å–", type="primary"):
                urls_to_crawl = []

                if url_input.strip():
                    urls_to_crawl.append(url_input.strip())

                if batch_urls.strip():
                    batch_list = [url.strip() for url in batch_urls.strip().split('\n') if url.strip()]
                    urls_to_crawl.extend(batch_list)

                if not urls_to_crawl:
                    st.warning("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªURL")
                else:
                    # å¼€å§‹çˆ¬å–
                    progress_bar = st.progress(0)
                    status_text = st.empty()

                    results = []

                    for i, url in enumerate(urls_to_crawl):
                        status_text.text(f"æ­£åœ¨çˆ¬å–: {url}")

                        def update_progress(message):
                            status_text.text(f"{url}: {message}")

                        result = crawler.crawl_url(url, update_progress)
                        results.append(result)

                        progress_bar.progress((i + 1) / len(urls_to_crawl))

                    # ä¿å­˜ç»“æœåˆ°session state
                    st.session_state.crawl_results = results

                    # æ˜¾ç¤ºç»“æœ
                    status_text.text("çˆ¬å–å®Œæˆ!")

                    success_count = sum(1 for r in results if r["success"])
                    st.success(f"âœ… æˆåŠŸçˆ¬å– {success_count}/{len(results)} ä¸ªé¡µé¢")

        # æ˜¾ç¤ºçˆ¬å–å†å²ç»“æœ
        if st.session_state.crawl_results:
            st.subheader("çˆ¬å–ç»“æœ")
            for result in st.session_state.crawl_results:
                url_key = url_to_key(result['url'])
                with st.expander(f"{'âœ…' if result['success'] else 'âŒ'} {result['url']}"):
                    if result["success"]:
                        st.write("**é¡µé¢ä¿¡æ¯:**")
                        metadata = result["metadata"]
                        col1, col2 = st.columns(2)
                        with col1:
                            st.write(f"- **æ ‡é¢˜**: {metadata.get('title', 'N/A')}")
                            st.write(f"- **è¯­è¨€**: {metadata.get('language', 'N/A')}")
                        with col2:
                            st.write(f"- **å†…å®¹é•¿åº¦**: {len(result['content'])} å­—ç¬¦")
                            st.write(f"- **çˆ¬å–æ—¶é—´**: {metadata.get('crawled_at', 'N/A')}")

                        st.write("**å­˜å‚¨çŠ¶æ€:**")
                        for service, status in result["storage_status"].items():
                            st.write(f"- {service}: {status}")

                        if st.button(f"æŸ¥çœ‹å†…å®¹", key=f"content_{url_key}"):
                            st.session_state.show_content[url_key] = True
                            st.rerun()

                        # æ˜¾ç¤ºå†…å®¹ï¼ˆå¦‚æœç”¨æˆ·ç‚¹å‡»äº†æŸ¥çœ‹å†…å®¹ï¼‰
                        if st.session_state.show_content.get(url_key, False):
                            st.text_area("å†…å®¹é¢„è§ˆ:", result["content"][:1000] + "..." if len(result["content"]) > 1000 else result["content"], height=200, key=f"content_display_{url_key}")
                            if st.button(f"éšè—å†…å®¹", key=f"hide_{url_key}"):
                                st.session_state.show_content[url_key] = False
                                st.rerun()
                    else:
                        st.error(f"çˆ¬å–å¤±è´¥: {result['error']}")

            # æ¸…ç©ºå†å²æŒ‰é’®
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºçˆ¬å–å†å²"):
                st.session_state.crawl_results = []
                st.session_state.show_content = {}
                st.rerun()

        with col2:
            if st.button("ğŸ§¹ æ¸…ç†æ‰€æœ‰æ•°æ®"):
                confirm = st.checkbox("ç¡®è®¤æ¸…ç†æ‰€æœ‰æ•°æ®")
                if confirm and st.button("ç¡®è®¤æ¸…ç†", type="secondary"):
                    try:
                        # æ¸…ç†æ‰€æœ‰å­˜å‚¨
                        crawler.engine.execute_query(f"TRUNCATE TABLE {crawler.doc_store.table_name}")
                        crawler.engine.execute_query(f"TRUNCATE TABLE {crawler.cache_store.table_name}")
                        # FileStore ä½¿ç”¨ Volume Storeï¼Œéœ€è¦ç”¨ä¸åŒçš„æ¸…ç†æ–¹æ³•
                        # crawler.engine.execute_query(f"TRUNCATE TABLE {crawler.file_store.table_name}")
                        if crawler.vector_store:
                            crawler.engine.execute_query(f"TRUNCATE TABLE {crawler.vector_store.table_name}")
                        st.success("âœ… æ‰€æœ‰æ•°æ®å·²æ¸…ç†")
                    except Exception as e:
                        st.error(f"æ¸…ç†å¤±è´¥: {e}")

    with tab2:
        st.header("å†…å®¹æœç´¢")

        search_query = st.text_input("è¯·è¾“å…¥æœç´¢å…³é”®è¯:", placeholder="è¾“å…¥è¦æœç´¢çš„å†…å®¹...")

        col1, col2 = st.columns(2)
        with col1:
            search_type = st.selectbox(
                "æœç´¢æ–¹å¼:",
                ["semantic", "keyword"],
                format_func=lambda x: "è¯­ä¹‰æœç´¢" if x == "semantic" else "å…³é”®è¯æœç´¢"
            )

        if st.button("ğŸ” æœç´¢", type="primary"):
            if search_query.strip():
                with st.spinner("æœç´¢ä¸­..."):
                    results = crawler.search_documents(search_query, search_type)

                if results:
                    st.success(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")

                    for i, result in enumerate(results):
                        with st.expander(f"ç»“æœ {i+1}: {result['metadata'].get('title', 'æ— æ ‡é¢˜')}"):
                            st.write(f"**æœç´¢æ–¹å¼**: {result['search_type']}")
                            st.write(f"**URL**: {result['metadata'].get('url', 'N/A')}")
                            st.write(f"**çˆ¬å–æ—¶é—´**: {result['metadata'].get('crawled_at', 'N/A')}")
                            st.write("**å†…å®¹é¢„è§ˆ**:")
                            st.text_area("æœç´¢ç»“æœå†…å®¹", result["content"], height=150, key=f"result_{i}", label_visibility="collapsed")
                else:
                    st.info("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ")
            else:
                st.warning("è¯·è¾“å…¥æœç´¢å…³é”®è¯")

    with tab3:
        st.header("å­˜å‚¨ç»Ÿè®¡")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶åˆ·æ–°ç»Ÿè®¡
        force_refresh = st.session_state.get('force_stats_refresh', False)
        if force_refresh:
            st.session_state.force_stats_refresh = False  # æ¸…é™¤æ ‡è®°
            with st.spinner("è‡ªåŠ¨åˆ·æ–°ç»Ÿè®¡ä¿¡æ¯..."):
                stats = crawler.get_storage_stats()
                st.info("ğŸ“Š ç»Ÿè®¡æ•°æ®å·²è‡ªåŠ¨æ›´æ–°")

        if st.button("ğŸ”„ åˆ·æ–°ç»Ÿè®¡") or force_refresh:
            if not force_refresh:  # é¿å…é‡å¤è·å–
                with st.spinner("è·å–ç»Ÿè®¡ä¿¡æ¯..."):
                    stats = crawler.get_storage_stats()

            if "error" not in stats:
                col1, col2, col3 = st.columns(3)

                with col1:
                    st.metric(
                        "æ–‡æ¡£æ€»æ•°",
                        stats.get("documents", {}).get("doc_count", 0)
                    )
                    avg_length = stats.get('documents', {}).get('avg_content_length', 0) or 0
                    st.metric(
                        "å¹³å‡æ–‡æ¡£é•¿åº¦",
                        f"{avg_length:.0f} å­—ç¬¦"
                    )

                with col2:
                    st.metric(
                        "ç¼“å­˜æ¡ç›®æ•°",
                        stats.get("cache", {}).get("cache_count", 0)
                    )

                with col3:
                    st.metric(
                        "æ–‡ä»¶æ€»æ•°",
                        stats.get("files", {}).get("file_count", 0)
                    )
                    file_size = stats.get("files", {}).get("total_size", 0)

                    if isinstance(file_size, (int, float)) and file_size > 0:
                        if file_size >= 1024 * 1024:  # >= 1MB
                            size_display = f"{file_size / 1024 / 1024:.2f} MB"
                        elif file_size >= 1024:  # >= 1KB
                            size_display = f"{file_size / 1024:.2f} KB"
                        else:  # < 1KB
                            size_display = f"{file_size} bytes"
                    elif isinstance(file_size, str):
                        size_display = file_size
                    else:
                        size_display = "0 bytes"

                    st.metric("æ€»æ–‡ä»¶å¤§å°", size_display)

                # å­˜å‚¨æœåŠ¡çŠ¶æ€
                st.subheader("å­˜å‚¨æœåŠ¡çŠ¶æ€")

                services = [
                    ("æ–‡æ¡£å­˜å‚¨", "ClickZettaDocumentStore", "å­˜å‚¨ç½‘é¡µå†…å®¹å’Œå…ƒæ•°æ®"),
                    ("é”®å€¼å­˜å‚¨", "ClickZettaStore", "å­˜å‚¨çˆ¬å–çŠ¶æ€å’Œç¼“å­˜"),
                    ("æ–‡ä»¶å­˜å‚¨", "ClickZettaFileStore", "å­˜å‚¨åŸå§‹HTMLæ–‡ä»¶"),
                    ("å‘é‡å­˜å‚¨", "ClickZettaVectorStore", "è¯­ä¹‰æœç´¢æ”¯æŒ")
                ]

                for service_name, class_name, description in services:
                    with st.expander(f"ğŸ“¦ {service_name} ({class_name})"):
                        st.write(description)

                        # æ˜¾ç¤ºå¯¹åº”çš„æ•°æ®é‡
                        if service_name == "æ–‡æ¡£å­˜å‚¨":
                            st.metric("å­˜å‚¨æ–‡æ¡£æ•°", stats.get("documents", {}).get("doc_count", 0))
                        elif service_name == "é”®å€¼å­˜å‚¨":
                            st.metric("ç¼“å­˜æ¡ç›®æ•°", stats.get("cache", {}).get("cache_count", 0))
                        elif service_name == "æ–‡ä»¶å­˜å‚¨":
                            st.metric("å­˜å‚¨æ–‡ä»¶æ•°", stats.get("files", {}).get("file_count", 0))
                        elif service_name == "å‘é‡å­˜å‚¨":
                            if crawler.vector_store:
                                st.write("âœ… å¯ç”¨ (DashScopeé›†æˆ)")
                            else:
                                st.write("âš ï¸ ä¸å¯ç”¨ (éœ€è¦DashScope APIå¯†é’¥)")
            else:
                st.error(f"è·å–ç»Ÿè®¡å¤±è´¥: {stats['error']}")

    with tab4:
        st.header("åŠŸèƒ½æ¼”ç¤º")

        st.markdown("""
        ### ğŸ¯ æ¼”ç¤ºåœºæ™¯

        è¿™ä¸ªç¤ºä¾‹å®Œæ•´å±•ç¤ºäº†ClickZettaå­˜å‚¨æœåŠ¡çš„å››å¤§æ ¸å¿ƒèƒ½åŠ›:
        """)

        # åŠŸèƒ½å¡ç‰‡
        features = [
            {
                "title": "ğŸ“š æ–‡æ¡£å­˜å‚¨ (ClickZettaDocumentStore)",
                "description": "å­˜å‚¨ç»“æ„åŒ–æ–‡æ¡£å†…å®¹å’Œå…ƒæ•°æ®ï¼Œæ”¯æŒSQLæŸ¥è¯¢",
                "example": "å­˜å‚¨ç½‘é¡µæ ‡é¢˜ã€å†…å®¹ã€ä½œè€…ã€å‘å¸ƒæ—¶é—´ç­‰ä¿¡æ¯",
                "benefits": ["SQLå¯æŸ¥è¯¢", "å…ƒæ•°æ®ä¸°å¯Œ", "ç»“æ„åŒ–å­˜å‚¨"]
            },
            {
                "title": "ğŸ—ƒï¸ é”®å€¼å­˜å‚¨ (ClickZettaStore)",
                "description": "é«˜æ€§èƒ½é”®å€¼å¯¹å­˜å‚¨ï¼Œé€‚åˆç¼“å­˜å’ŒçŠ¶æ€ç®¡ç†",
                "example": "å­˜å‚¨çˆ¬å–çŠ¶æ€ã€å†…å®¹å“ˆå¸Œã€æœ€åæ›´æ–°æ—¶é—´",
                "benefits": ["é«˜æ€§èƒ½è¯»å†™", "åŸå­æ“ä½œ", "æ‰¹é‡å¤„ç†"]
            },
            {
                "title": "ğŸ“ æ–‡ä»¶å­˜å‚¨ (ClickZettaFileStore)",
                "description": "åŸºäºClickZetta Volumeçš„äºŒè¿›åˆ¶æ–‡ä»¶å­˜å‚¨",
                "example": "å­˜å‚¨åŸå§‹HTMLæ–‡ä»¶ã€å›¾ç‰‡ã€PDFç­‰",
                "benefits": ["äºŒè¿›åˆ¶æ”¯æŒ", "å¤§æ–‡ä»¶ä¼˜åŒ–", "ç‰ˆæœ¬ç®¡ç†"]
            },
            {
                "title": "ğŸ” å‘é‡å­˜å‚¨ (ClickZettaVectorStore)",
                "description": "æ”¯æŒè¯­ä¹‰æœç´¢çš„å‘é‡æ•°æ®åº“åŠŸèƒ½",
                "example": "åŸºäºå†…å®¹ç›¸ä¼¼æ€§çš„æ™ºèƒ½æ–‡æ¡£æ£€ç´¢",
                "benefits": ["è¯­ä¹‰ç†è§£", "ç›¸ä¼¼æ€§æœç´¢", "AIé©±åŠ¨"]
            }
        ]

        for i, feature in enumerate(features):
            with st.expander(f"{feature['title']}", expanded=i==0):
                st.write(feature['description'])
                st.write(f"**ç¤ºä¾‹ç”¨é€”**: {feature['example']}")

                col1, col2 = st.columns([2, 1])
                with col1:
                    st.write("**ä¸»è¦ä¼˜åŠ¿**:")
                    for benefit in feature['benefits']:
                        st.write(f"- {benefit}")

                with col2:
                    if i == 0:  # æ–‡æ¡£å­˜å‚¨
                        st.code('''
doc_store.store_document(
    key=url_hash,
    content=page_content,
    metadata={
        "title": title,
        "url": url,
        "crawled_at": timestamp
    }
)''')
                    elif i == 1:  # é”®å€¼å­˜å‚¨
                        st.code('''
cache_store.mset([
    ("status:123", b"completed"),
    ("hash:123", content_hash),
    ("time:123", timestamp)
])''')
                    elif i == 2:  # æ–‡ä»¶å­˜å‚¨
                        st.code('''
file_store.store_file(
    key="page.html",
    content=raw_html,
    content_type="text/html"
)''')
                    else:  # å‘é‡å­˜å‚¨
                        st.code('''
vector_store.add_documents([
    Document(
        page_content=content,
        metadata=metadata
    )
])''')

        st.markdown("""
        ### ğŸš€ æŠ€æœ¯ä¼˜åŠ¿

        1. **ç»Ÿä¸€å¹³å°**: æ‰€æœ‰å­˜å‚¨éœ€æ±‚åœ¨ä¸€ä¸ªClickZettaå¹³å°è§£å†³
        2. **ACIDäº‹åŠ¡**: ç¡®ä¿æ•°æ®ä¸€è‡´æ€§å’Œå¯é æ€§
        3. **é«˜æ€§èƒ½**: ClickZettaçš„äº‘åŸç”Ÿæ¶æ„æä¾›10å€æ€§èƒ½æå‡
        4. **æ˜“æ‰©å±•**: ä»GBåˆ°PBçº§æ•°æ®çš„æ— ç¼æ‰©å±•
        5. **SQLå…¼å®¹**: ç†Ÿæ‚‰çš„SQLæŸ¥è¯¢è¯­æ³•
        6. **AIå°±ç»ª**: åŸç”Ÿæ”¯æŒå‘é‡æœç´¢å’Œæœºå™¨å­¦ä¹ 
        """)

        st.markdown("""
        ### ğŸ“ˆ å®é™…åº”ç”¨åœºæ™¯

        - **ä¼ä¸šçŸ¥è¯†åº“**: çˆ¬å–å¹¶å­˜å‚¨å†…éƒ¨æ–‡æ¡£ã€æ”¿ç­–ã€æµç¨‹
        - **ç«å“åˆ†æ**: ç›‘æ§ç«äº‰å¯¹æ‰‹ç½‘ç«™å†…å®¹å˜åŒ–
        - **æ–°é—»èšåˆ**: æ”¶é›†å¤šæºæ–°é—»è¿›è¡Œåˆ†æå’Œæ¨è
        - **å­¦æœ¯ç ”ç©¶**: æ”¶é›†è®ºæ–‡ã€æŠ¥å‘Šè¿›è¡Œæ–‡çŒ®ç»¼è¿°
        - **ç”µå•†ç›‘æ§**: è·Ÿè¸ªäº§å“ä»·æ ¼ã€è¯„ä»·ã€åº“å­˜å˜åŒ–
        """)

if __name__ == "__main__":
    main()