"""
ClickZetta Web Crawler & Storage Demo

å±•ç¤ºLangChainç½‘ç«™çˆ¬å–åŠŸèƒ½ä¸ŽClickZettaå­˜å‚¨æœåŠ¡çš„å®Œæ•´é›†æˆã€‚
æ¼”ç¤ºæ–‡æ¡£å­˜å‚¨ã€é”®å€¼å­˜å‚¨ã€æ–‡ä»¶å­˜å‚¨å’Œå‘é‡å­˜å‚¨çš„å®žé™…åº”ç”¨ã€‚
"""

import streamlit as st
import sys
import os
import hashlib
import json
import time
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from urllib.parse import urljoin, urlparse
import re

try:
    import requests
    from bs4 import BeautifulSoup
    import html2text
    import validators
    from langchain_core.documents import Document
    from langchain_community.embeddings import DashScopeEmbeddings
    from langchain_clickzetta import (
        ClickZettaEngine,
        ClickZettaStore,
        ClickZettaDocumentStore,
        ClickZettaFileStore,
        ClickZettaVectorStore
    )
    from dotenv import load_dotenv
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    DEPENDENCIES_AVAILABLE = False
    import_error = str(e)

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="ClickZetta Web Crawler & Storage Demo",
    page_icon="ðŸ•·ï¸",
    layout="wide"
)

def extract_text_content(html_content: str) -> str:
    """ä»ŽHTMLä¸­æå–çº¯æ–‡æœ¬å†…å®¹"""
    if not DEPENDENCIES_AVAILABLE:
        return "ä¾èµ–æœªå®‰è£…"

    try:
        # ä½¿ç”¨html2textè½¬æ¢HTMLä¸ºMarkdownæ ¼å¼æ–‡æœ¬
        h = html2text.HTML2Text()
        h.ignore_links = False
        h.ignore_images = True
        h.ignore_emphasis = False
        text_content = h.handle(html_content)

        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        lines = [line.strip() for line in text_content.split('\n')]
        cleaned_lines = []
        for line in lines:
            if line or (cleaned_lines and cleaned_lines[-1]):
                cleaned_lines.append(line)

        return '\n'.join(cleaned_lines)
    except Exception as e:
        st.warning(f"æ–‡æœ¬æå–å¤±è´¥: {e}")
        # fallback to BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        return soup.get_text()

def extract_metadata(soup, url: str) -> Dict:
    """æå–ç½‘é¡µå…ƒæ•°æ®"""
    metadata = {
        "url": url,
        "crawled_at": datetime.now().isoformat(),
        "title": "",
        "description": "",
        "author": "",
        "keywords": [],
        "language": "zh",
        "content_type": "webpage"
    }

    try:
        # æå–æ ‡é¢˜
        title_tag = soup.find('title')
        if title_tag:
            metadata["title"] = title_tag.get_text().strip()

        # æå–æè¿°
        desc_tag = soup.find('meta', attrs={'name': 'description'})
        if desc_tag:
            metadata["description"] = desc_tag.get('content', '').strip()

        # æå–ä½œè€…
        author_tag = soup.find('meta', attrs={'name': 'author'})
        if author_tag:
            metadata["author"] = author_tag.get('content', '').strip()

        # æå–å…³é”®è¯
        keywords_tag = soup.find('meta', attrs={'name': 'keywords'})
        if keywords_tag:
            keywords = keywords_tag.get('content', '').strip()
            metadata["keywords"] = [k.strip() for k in keywords.split(',') if k.strip()]

        # æ£€æµ‹è¯­è¨€
        html_tag = soup.find('html')
        if html_tag and html_tag.get('lang'):
            metadata["language"] = html_tag.get('lang')

    except Exception as e:
        st.warning(f"å…ƒæ•°æ®æå–å¤±è´¥: {e}")

    return metadata

def url_to_key(url: str) -> str:
    """å°†URLè½¬æ¢ä¸ºå­˜å‚¨é”®"""
    return hashlib.md5(url.encode()).hexdigest()

def load_env_config():
    """åŠ è½½çŽ¯å¢ƒé…ç½®"""
    load_dotenv()
    return {
        'clickzetta_service': os.getenv('CLICKZETTA_SERVICE', ''),
        'clickzetta_instance': os.getenv('CLICKZETTA_INSTANCE', ''),
        'clickzetta_workspace': os.getenv('CLICKZETTA_WORKSPACE', ''),
        'clickzetta_schema': os.getenv('CLICKZETTA_SCHEMA', ''),
        'clickzetta_username': os.getenv('CLICKZETTA_USERNAME', ''),
        'clickzetta_password': os.getenv('CLICKZETTA_PASSWORD', ''),
        'clickzetta_vcluster': os.getenv('CLICKZETTA_VCLUSTER', ''),
        'dashscope_api_key': os.getenv('DASHSCOPE_API_KEY', ''),
    }

def check_environment():
    """æ£€æŸ¥çŽ¯å¢ƒé…ç½®"""
    config = load_env_config()

    clickzetta_available = all([
        config['clickzetta_service'], config['clickzetta_instance'],
        config['clickzetta_workspace'], config['clickzetta_schema'],
        config['clickzetta_username'], config['clickzetta_password'],
        config['clickzetta_vcluster']
    ])

    dashscope_available = bool(config['dashscope_api_key'])

    return {
        "clickzetta_available": clickzetta_available,
        "dashscope_available": dashscope_available,
        "config": config
    }

def display_environment_status(env_status):
    """æ˜¾ç¤ºçŽ¯å¢ƒçŠ¶æ€"""
    col1, col2 = st.columns(2)

    with col1:
        if env_status["clickzetta_available"]:
            st.success("âœ… ClickZetta é…ç½®å®Œæ•´")
        else:
            st.error("âŒ ClickZetta é…ç½®ä¸å®Œæ•´")

    with col2:
        if env_status["dashscope_available"]:
            st.success("âœ… DashScope API é…ç½®å®Œæ•´")
            st.session_state['dashscope_available'] = True
        else:
            st.warning("âš ï¸ DashScope API æœªé…ç½®")
            st.session_state['dashscope_available'] = False

def create_sidebar_info():
    """åˆ›å»ºä¾§è¾¹æ ä¿¡æ¯"""
    st.sidebar.header("â„¹ï¸ åº”ç”¨ä¿¡æ¯")
    st.sidebar.markdown("""
    ### ðŸ•·ï¸ ç½‘ç»œçˆ¬è™«æ¼”ç¤º

    **åŠŸèƒ½ç‰¹è‰²:**
    - ç½‘é¡µå†…å®¹çˆ¬å–
    - å¤šç§å­˜å‚¨æœåŠ¡
    - è¯­ä¹‰æœç´¢
    - ç»Ÿè®¡åˆ†æž

    **å­˜å‚¨æœåŠ¡:**
    - DocumentStore: æ–‡æ¡£å­˜å‚¨
    - Store: é”®å€¼å­˜å‚¨
    - FileStore: æ–‡ä»¶å­˜å‚¨
    - VectorStore: å‘é‡å­˜å‚¨
    """)

def get_clickzetta_engine():
    """èŽ·å–ClickZettaå¼•æ“Ž"""
    config = load_env_config()
    return ClickZettaEngine(
        service=config['clickzetta_service'],
        instance=config['clickzetta_instance'],
        workspace=config['clickzetta_workspace'],
        schema=config['clickzetta_schema'],
        username=config['clickzetta_username'],
        password=config['clickzetta_password'],
        vcluster=config['clickzetta_vcluster']
    )

class WebCrawlerDemo:
    """ç½‘ç»œçˆ¬è™«æ¼”ç¤ºç±»"""

    def __init__(self, engine):
        self.engine = engine
        self.setup_storage_services()

    def setup_storage_services(self):
        """åˆå§‹åŒ–æ‰€æœ‰å­˜å‚¨æœåŠ¡"""
        try:
            # æ–‡æ¡£å­˜å‚¨ - å­˜å‚¨ç½‘é¡µå†…å®¹å’Œå…ƒæ•°æ®
            self.doc_store = ClickZettaDocumentStore(
                engine=self.engine,
                table_name="web_crawler_documents"
            )

            # é”®å€¼å­˜å‚¨ - å­˜å‚¨çˆ¬å–çŠ¶æ€å’Œç¼“å­˜
            self.cache_store = ClickZettaStore(
                engine=self.engine,
                table_name="web_crawler_cache"
            )

            # æ–‡ä»¶å­˜å‚¨ - å­˜å‚¨åŽŸå§‹HTMLæ–‡ä»¶
            self.file_store = ClickZettaFileStore(
                engine=self.engine,
                volume_type="user",
                subdirectory="web_crawler_files"
            )

            # å‘é‡å­˜å‚¨ - è¯­ä¹‰æœç´¢
            if st.session_state.get('dashscope_available'):
                embeddings = DashScopeEmbeddings(
                    dashscope_api_key=os.getenv("DASHSCOPE_API_KEY"),
                    model="text-embedding-v4"
                )
                self.vector_store = ClickZettaVectorStore(
                    engine=self.engine,
                    embeddings=embeddings,
                    table_name="web_crawler_vectors"
                )
            else:
                self.vector_store = None

        except Exception as e:
            st.error(f"å­˜å‚¨æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")

    def crawl_url(self, url: str, progress_callback=None) -> Dict:
        """çˆ¬å–å•ä¸ªURL"""
        result = {
            "success": False,
            "url": url,
            "error": None,
            "content": "",
            "metadata": {},
            "storage_status": {}
        }

        try:
            # éªŒè¯URL
            if not validators.url(url):
                result["error"] = "æ— æ•ˆçš„URLæ ¼å¼"
                return result

            if progress_callback:
                progress_callback("æ­£åœ¨å‘é€è¯·æ±‚...")

            # å‘é€HTTPè¯·æ±‚
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()

            if progress_callback:
                progress_callback("æ­£åœ¨è§£æžHTML...")

            # è§£æžHTML
            soup = BeautifulSoup(response.content, 'html.parser')

            # æå–å†…å®¹å’Œå…ƒæ•°æ®
            text_content = extract_text_content(response.text)
            metadata = extract_metadata(soup, url)

            # è®¡ç®—URLé”®
            url_key = url_to_key(url)

            if progress_callback:
                progress_callback("æ­£åœ¨å­˜å‚¨åˆ°ClickZetta...")

            # å­˜å‚¨åˆ°å„ç§å­˜å‚¨æœåŠ¡
            storage_status = self.store_crawled_data(
                url_key, url, text_content, metadata, response.text
            )

            result.update({
                "success": True,
                "content": text_content,
                "metadata": metadata,
                "storage_status": storage_status
            })

        except requests.RequestException as e:
            result["error"] = f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}"
        except Exception as e:
            result["error"] = f"çˆ¬å–å¤±è´¥: {e}"

        return result

    def store_crawled_data(self, url_key: str, url: str, content: str,
                          metadata: Dict, raw_html: str) -> Dict:
        """å°†çˆ¬å–çš„æ•°æ®å­˜å‚¨åˆ°å„ç§å­˜å‚¨æœåŠ¡"""
        storage_status = {}

        try:
            # 1. æ–‡æ¡£å­˜å‚¨ - å­˜å‚¨å¤„ç†åŽçš„å†…å®¹å’Œå…ƒæ•°æ®
            self.doc_store.store_document(
                doc_id=url_key,
                content=content,
                metadata=metadata
            )
            storage_status["document_store"] = "âœ… æˆåŠŸ"

            # 2. é”®å€¼å­˜å‚¨ - å­˜å‚¨çˆ¬å–çŠ¶æ€
            cache_data = [
                (f"crawl_status:{url_key}", b"completed"),
                (f"url_mapping:{url_key}", url.encode()),
                (f"crawl_time:{url_key}", datetime.now().isoformat().encode()),
                (f"content_hash:{url_key}", hashlib.md5(content.encode()).hexdigest().encode())
            ]
            self.cache_store.mset(cache_data)
            storage_status["cache_store"] = "âœ… æˆåŠŸ"

            # 3. æ–‡ä»¶å­˜å‚¨ - å­˜å‚¨åŽŸå§‹HTML
            self.file_store.store_file(
                file_path=f"{url_key}.html",
                content=raw_html.encode(),
                mime_type="text/html"
            )
            storage_status["file_store"] = "âœ… æˆåŠŸ"

            # 4. å‘é‡å­˜å‚¨ - è¯­ä¹‰æœç´¢ï¼ˆå¦‚æžœå¯ç”¨ï¼‰
            if self.vector_store and content.strip():
                doc = Document(
                    page_content=content,
                    metadata=metadata
                )
                self.vector_store.add_documents([doc])
                storage_status["vector_store"] = "âœ… æˆåŠŸ"
            else:
                storage_status["vector_store"] = "âš ï¸ è·³è¿‡ (DashScopeä¸å¯ç”¨æˆ–å†…å®¹ä¸ºç©º)"

        except Exception as e:
            storage_status["error"] = f"âŒ å­˜å‚¨å¤±è´¥: {e}"

        return storage_status

    def search_documents(self, query: str, search_type: str = "semantic") -> List[Dict]:
        """æœç´¢å·²çˆ¬å–çš„æ–‡æ¡£"""
        results = []

        try:
            if search_type == "semantic" and self.vector_store:
                # è¯­ä¹‰æœç´¢
                docs = self.vector_store.similarity_search(query, k=5)
                for doc in docs:
                    results.append({
                        "content": doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content,
                        "metadata": doc.metadata,
                        "search_type": "è¯­ä¹‰æœç´¢"
                    })

            elif search_type == "keyword":
                # å…³é”®è¯æœç´¢ (ç®€å•å®žçŽ°)
                # è¿™é‡Œå¯ä»¥ä½¿ç”¨ClickZettaçš„å…¨æ–‡æœç´¢åŠŸèƒ½
                query_result, _ = self.engine.execute_query(f"""
                    SELECT doc_id, doc_content, metadata
                    FROM {self.doc_store.table_name}
                    WHERE doc_content LIKE '%{query}%'
                    LIMIT 5
                """)

                for row in query_result:
                    try:
                        metadata = json.loads(row['metadata']) if row['metadata'] else {}
                    except:
                        metadata = {}

                    results.append({
                        "content": row['doc_content'][:500] + "..." if len(row['doc_content']) > 500 else row['doc_content'],
                        "metadata": metadata,
                        "search_type": "å…³é”®è¯æœç´¢"
                    })

        except Exception as e:
            st.error(f"æœç´¢å¤±è´¥: {e}")

        return results

    def get_storage_stats(self) -> Dict:
        """èŽ·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}

        try:
            # æ–‡æ¡£ç»Ÿè®¡
            doc_result, _ = self.engine.execute_query(f"""
                SELECT COUNT(*) as doc_count,
                       AVG(LENGTH(doc_content)) as avg_content_length
                FROM {self.doc_store.table_name}
            """)
            stats["documents"] = doc_result[0] if doc_result else {"doc_count": 0, "avg_content_length": 0}

            # ç¼“å­˜ç»Ÿè®¡
            cache_result, _ = self.engine.execute_query(f"""
                SELECT COUNT(*) as cache_count
                FROM {self.cache_store.table_name}
            """)
            stats["cache"] = cache_result[0] if cache_result else {"cache_count": 0}

            # æ–‡ä»¶ç»Ÿè®¡ (é€šè¿‡ Volume Store API)
            try:
                # list_files() è¿”å›ž list[tuple[str, int, str]] æ ¼å¼: (æ–‡ä»¶è·¯å¾„, æ–‡ä»¶å¤§å°, å†…å®¹ç±»åž‹)
                files = self.file_store.list_files()
                file_count = len(files) if files else 0
                total_size = 0
                if files:
                    for file_info in files:
                        if isinstance(file_info, tuple) and len(file_info) >= 2:
                            # file_info[1] æ˜¯æ–‡ä»¶å¤§å°
                            file_size = file_info[1]
                            total_size += file_size
                            # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ¯ä¸ªæ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯
                            # st.write(f"æ–‡ä»¶: {file_info[0]}, å¤§å°: {file_size} bytes")

                stats["files"] = {
                    "file_count": file_count,
                    "total_size": total_size,
                    "total_size_kb": round(total_size / 1024, 2) if total_size > 0 else 0
                }
            except Exception as e:
                stats["files"] = {"file_count": f"èŽ·å–å¤±è´¥: {e}", "total_size": 0}

        except Exception as e:
            st.warning(f"ç»Ÿè®¡ä¿¡æ¯èŽ·å–å¤±è´¥: {e}")
            stats = {"error": str(e)}

        return stats

def main():
    """ä¸»å‡½æ•°"""
    st.title("ðŸ•·ï¸ ClickZetta Web Crawler & Storage Demo")
    st.markdown("### å±•ç¤ºLangChainç½‘ç«™çˆ¬å–ä¸ŽClickZettaå­˜å‚¨æœåŠ¡çš„å®Œæ•´é›†æˆ")

    # åˆå§‹åŒ–session state
    if 'crawl_results' not in st.session_state:
        st.session_state.crawl_results = []
    if 'show_content' not in st.session_state:
        st.session_state.show_content = {}

    # æ£€æŸ¥ä¾èµ–
    if not DEPENDENCIES_AVAILABLE:
        st.error(f"ç¼ºå°‘å¿…è¦ä¾èµ–: {import_error}")
        st.info("è¯·è¿è¡Œ: pip install -r requirements.txt")
        return

    # æ£€æŸ¥çŽ¯å¢ƒé…ç½®
    env_status = check_environment()
    display_environment_status(env_status)

    if not env_status["clickzetta_available"]:
        st.error("ClickZettaé…ç½®ä¸å®Œæ•´ï¼Œæ— æ³•ä½¿ç”¨å­˜å‚¨æœåŠ¡")
        return

    # åˆ›å»ºä¾§è¾¹æ ä¿¡æ¯
    create_sidebar_info()

    # åˆå§‹åŒ–ClickZettaå¼•æ“Ž
    try:
        engine = get_clickzetta_engine()
        crawler = WebCrawlerDemo(engine)
        st.success("âœ… ClickZettaå­˜å‚¨æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        st.error(f"âŒ ClickZettaè¿žæŽ¥å¤±è´¥: {e}")
        return

    # ä¸»ç•Œé¢æ ‡ç­¾é¡µ
    tab1, tab2, tab3, tab4 = st.tabs(["ðŸ•·ï¸ ç½‘é¡µçˆ¬å–", "ðŸ” å†…å®¹æœç´¢", "ðŸ“Š å­˜å‚¨ç»Ÿè®¡", "ðŸ’¡ åŠŸèƒ½æ¼”ç¤º"])

    with tab1:
        st.header("ç½‘é¡µçˆ¬å–ä¸Žå­˜å‚¨")

        # URLè¾“å…¥
        url_input = st.text_input(
            "è¯·è¾“å…¥è¦çˆ¬å–çš„ç½‘é¡µURL:",
            value="https://www.yunqi.tech",
            placeholder="https://www.yunqi.tech",
            help="æ”¯æŒå¤§å¤šæ•°ç½‘ç«™ï¼Œå»ºè®®ä½¿ç”¨æ–°é—»ã€åšå®¢ç­‰å†…å®¹ä¸°å¯Œçš„é¡µé¢"
        )

        # æ‰¹é‡URLè¾“å…¥
        with st.expander("æ‰¹é‡çˆ¬å– (æ¯è¡Œä¸€ä¸ªURL)"):
            batch_urls = st.text_area(
                "æ‰¹é‡URLåˆ—è¡¨:",
                placeholder="https://www.yunqi.tech\nhttps://www.yunqi.tech/products\nhttps://www.yunqi.tech/solutions",
                height=100
            )

        col1, col2 = st.columns(2)

        with col1:
            if st.button("ðŸš€ å¼€å§‹çˆ¬å–", type="primary"):
                urls_to_crawl = []

                if url_input.strip():
                    urls_to_crawl.append(url_input.strip())

                if batch_urls.strip():
                    batch_list = [url.strip() for url in batch_urls.strip().split('\n') if url.strip()]
                    urls_to_crawl.extend(batch_list)

                if not urls_to_crawl:
                    st.warning("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªURL")
                else:
                    # å¼€å§‹çˆ¬å–
                    progress_bar = st.progress(0)
                    status_text = st.empty()

                    results = []

                    for i, url in enumerate(urls_to_crawl):
                        status_text.text(f"æ­£åœ¨çˆ¬å–: {url}")

                        def update_progress(message):
                            status_text.text(f"{url}: {message}")

                        result = crawler.crawl_url(url, update_progress)
                        results.append(result)

                        progress_bar.progress((i + 1) / len(urls_to_crawl))

                    # ä¿å­˜ç»“æžœåˆ°session state
                    st.session_state.crawl_results = results

                    # æ˜¾ç¤ºç»“æžœ
                    status_text.text("çˆ¬å–å®Œæˆ!")

                    success_count = sum(1 for r in results if r["success"])
                    st.success(f"âœ… æˆåŠŸçˆ¬å– {success_count}/{len(results)} ä¸ªé¡µé¢")

        # æ˜¾ç¤ºçˆ¬å–åŽ†å²ç»“æžœ
        if st.session_state.crawl_results:
            st.subheader("çˆ¬å–ç»“æžœ")
            for result in st.session_state.crawl_results:
                url_key = url_to_key(result['url'])
                with st.expander(f"{'âœ…' if result['success'] else 'âŒ'} {result['url']}"):
                    if result["success"]:
                        st.write("**é¡µé¢ä¿¡æ¯:**")
                        metadata = result["metadata"]
                        col1, col2 = st.columns(2)
                        with col1:
                            st.write(f"- **æ ‡é¢˜**: {metadata.get('title', 'N/A')}")
                            st.write(f"- **è¯­è¨€**: {metadata.get('language', 'N/A')}")
                        with col2:
                            st.write(f"- **å†…å®¹é•¿åº¦**: {len(result['content'])} å­—ç¬¦")
                            st.write(f"- **çˆ¬å–æ—¶é—´**: {metadata.get('crawled_at', 'N/A')}")

                        st.write("**å­˜å‚¨çŠ¶æ€:**")
                        for service, status in result["storage_status"].items():
                            st.write(f"- {service}: {status}")

                        if st.button(f"æŸ¥çœ‹å†…å®¹", key=f"content_{url_key}"):
                            st.session_state.show_content[url_key] = True
                            st.rerun()

                        # æ˜¾ç¤ºå†…å®¹ï¼ˆå¦‚æžœç”¨æˆ·ç‚¹å‡»äº†æŸ¥çœ‹å†…å®¹ï¼‰
                        if st.session_state.show_content.get(url_key, False):
                            st.text_area("å†…å®¹é¢„è§ˆ:", result["content"][:1000] + "..." if len(result["content"]) > 1000 else result["content"], height=200, key=f"content_display_{url_key}")
                            if st.button(f"éšè—å†…å®¹", key=f"hide_{url_key}"):
                                st.session_state.show_content[url_key] = False
                                st.rerun()
                    else:
                        st.error(f"çˆ¬å–å¤±è´¥: {result['error']}")

            # æ¸…ç©ºåŽ†å²æŒ‰é’®
            if st.button("ðŸ—‘ï¸ æ¸…ç©ºçˆ¬å–åŽ†å²"):
                st.session_state.crawl_results = []
                st.session_state.show_content = {}
                st.rerun()

        with col2:
            if st.button("ðŸ§¹ æ¸…ç†æ‰€æœ‰æ•°æ®"):
                confirm = st.checkbox("ç¡®è®¤æ¸…ç†æ‰€æœ‰æ•°æ®")
                if confirm and st.button("ç¡®è®¤æ¸…ç†", type="secondary"):
                    try:
                        # æ¸…ç†æ‰€æœ‰å­˜å‚¨
                        crawler.engine.execute_query(f"TRUNCATE TABLE {crawler.doc_store.table_name}")
                        crawler.engine.execute_query(f"TRUNCATE TABLE {crawler.cache_store.table_name}")
                        # FileStore ä½¿ç”¨ Volume Storeï¼Œéœ€è¦ç”¨ä¸åŒçš„æ¸…ç†æ–¹æ³•
                        # crawler.engine.execute_query(f"TRUNCATE TABLE {crawler.file_store.table_name}")
                        if crawler.vector_store:
                            crawler.engine.execute_query(f"TRUNCATE TABLE {crawler.vector_store.table_name}")
                        st.success("âœ… æ‰€æœ‰æ•°æ®å·²æ¸…ç†")
                    except Exception as e:
                        st.error(f"æ¸…ç†å¤±è´¥: {e}")

    with tab2:
        st.header("å†…å®¹æœç´¢")

        search_query = st.text_input("è¯·è¾“å…¥æœç´¢å…³é”®è¯:", placeholder="è¾“å…¥è¦æœç´¢çš„å†…å®¹...")

        col1, col2 = st.columns(2)
        with col1:
            search_type = st.selectbox(
                "æœç´¢æ–¹å¼:",
                ["semantic", "keyword"],
                format_func=lambda x: "è¯­ä¹‰æœç´¢" if x == "semantic" else "å…³é”®è¯æœç´¢"
            )

        if st.button("ðŸ” æœç´¢", type="primary"):
            if search_query.strip():
                with st.spinner("æœç´¢ä¸­..."):
                    results = crawler.search_documents(search_query, search_type)

                if results:
                    st.success(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æžœ")

                    for i, result in enumerate(results):
                        with st.expander(f"ç»“æžœ {i+1}: {result['metadata'].get('title', 'æ— æ ‡é¢˜')}"):
                            st.write(f"**æœç´¢æ–¹å¼**: {result['search_type']}")
                            st.write(f"**URL**: {result['metadata'].get('url', 'N/A')}")
                            st.write(f"**çˆ¬å–æ—¶é—´**: {result['metadata'].get('crawled_at', 'N/A')}")
                            st.write("**å†…å®¹é¢„è§ˆ**:")
                            st.text_area("æœç´¢ç»“æžœå†…å®¹", result["content"], height=150, key=f"result_{i}", label_visibility="collapsed")
                else:
                    st.info("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æžœ")
            else:
                st.warning("è¯·è¾“å…¥æœç´¢å…³é”®è¯")

    with tab3:
        st.header("å­˜å‚¨ç»Ÿè®¡")

        if st.button("ðŸ”„ åˆ·æ–°ç»Ÿè®¡"):
            with st.spinner("èŽ·å–ç»Ÿè®¡ä¿¡æ¯..."):
                stats = crawler.get_storage_stats()

            if "error" not in stats:
                col1, col2, col3 = st.columns(3)

                with col1:
                    st.metric(
                        "æ–‡æ¡£æ€»æ•°",
                        stats.get("documents", {}).get("doc_count", 0)
                    )
                    st.metric(
                        "å¹³å‡æ–‡æ¡£é•¿åº¦",
                        f"{stats.get('documents', {}).get('avg_content_length', 0):.0f} å­—ç¬¦"
                    )

                with col2:
                    st.metric(
                        "ç¼“å­˜æ¡ç›®æ•°",
                        stats.get("cache", {}).get("cache_count", 0)
                    )

                with col3:
                    st.metric(
                        "æ–‡ä»¶æ€»æ•°",
                        stats.get("files", {}).get("file_count", 0)
                    )
                    file_size = stats.get("files", {}).get("total_size", 0)

                    if isinstance(file_size, (int, float)) and file_size > 0:
                        if file_size >= 1024 * 1024:  # >= 1MB
                            size_display = f"{file_size / 1024 / 1024:.2f} MB"
                        elif file_size >= 1024:  # >= 1KB
                            size_display = f"{file_size / 1024:.2f} KB"
                        else:  # < 1KB
                            size_display = f"{file_size} bytes"
                    elif isinstance(file_size, str):
                        size_display = file_size
                    else:
                        size_display = "0 bytes"

                    st.metric("æ€»æ–‡ä»¶å¤§å°", size_display)

                # å­˜å‚¨æœåŠ¡çŠ¶æ€
                st.subheader("å­˜å‚¨æœåŠ¡çŠ¶æ€")

                services = [
                    ("æ–‡æ¡£å­˜å‚¨", "ClickZettaDocumentStore", "å­˜å‚¨ç½‘é¡µå†…å®¹å’Œå…ƒæ•°æ®"),
                    ("é”®å€¼å­˜å‚¨", "ClickZettaStore", "å­˜å‚¨çˆ¬å–çŠ¶æ€å’Œç¼“å­˜"),
                    ("æ–‡ä»¶å­˜å‚¨", "ClickZettaFileStore", "å­˜å‚¨åŽŸå§‹HTMLæ–‡ä»¶"),
                    ("å‘é‡å­˜å‚¨", "ClickZettaVectorStore", "è¯­ä¹‰æœç´¢æ”¯æŒ")
                ]

                for service_name, class_name, description in services:
                    with st.expander(f"ðŸ“¦ {service_name} ({class_name})"):
                        st.write(description)

                        # æ˜¾ç¤ºå¯¹åº”çš„æ•°æ®é‡
                        if service_name == "æ–‡æ¡£å­˜å‚¨":
                            st.metric("å­˜å‚¨æ–‡æ¡£æ•°", stats.get("documents", {}).get("doc_count", 0))
                        elif service_name == "é”®å€¼å­˜å‚¨":
                            st.metric("ç¼“å­˜æ¡ç›®æ•°", stats.get("cache", {}).get("cache_count", 0))
                        elif service_name == "æ–‡ä»¶å­˜å‚¨":
                            st.metric("å­˜å‚¨æ–‡ä»¶æ•°", stats.get("files", {}).get("file_count", 0))
                        elif service_name == "å‘é‡å­˜å‚¨":
                            if crawler.vector_store:
                                st.write("âœ… å¯ç”¨ (DashScopeé›†æˆ)")
                            else:
                                st.write("âš ï¸ ä¸å¯ç”¨ (éœ€è¦DashScope APIå¯†é’¥)")
            else:
                st.error(f"èŽ·å–ç»Ÿè®¡å¤±è´¥: {stats['error']}")

    with tab4:
        st.header("åŠŸèƒ½æ¼”ç¤º")

        st.markdown("""
        ### ðŸŽ¯ æ¼”ç¤ºåœºæ™¯

        è¿™ä¸ªç¤ºä¾‹å®Œæ•´å±•ç¤ºäº†ClickZettaå­˜å‚¨æœåŠ¡çš„å››å¤§æ ¸å¿ƒèƒ½åŠ›:
        """)

        # åŠŸèƒ½å¡ç‰‡
        features = [
            {
                "title": "ðŸ“š æ–‡æ¡£å­˜å‚¨ (ClickZettaDocumentStore)",
                "description": "å­˜å‚¨ç»“æž„åŒ–æ–‡æ¡£å†…å®¹å’Œå…ƒæ•°æ®ï¼Œæ”¯æŒSQLæŸ¥è¯¢",
                "example": "å­˜å‚¨ç½‘é¡µæ ‡é¢˜ã€å†…å®¹ã€ä½œè€…ã€å‘å¸ƒæ—¶é—´ç­‰ä¿¡æ¯",
                "benefits": ["SQLå¯æŸ¥è¯¢", "å…ƒæ•°æ®ä¸°å¯Œ", "ç»“æž„åŒ–å­˜å‚¨"]
            },
            {
                "title": "ðŸ—ƒï¸ é”®å€¼å­˜å‚¨ (ClickZettaStore)",
                "description": "é«˜æ€§èƒ½é”®å€¼å¯¹å­˜å‚¨ï¼Œé€‚åˆç¼“å­˜å’ŒçŠ¶æ€ç®¡ç†",
                "example": "å­˜å‚¨çˆ¬å–çŠ¶æ€ã€å†…å®¹å“ˆå¸Œã€æœ€åŽæ›´æ–°æ—¶é—´",
                "benefits": ["é«˜æ€§èƒ½è¯»å†™", "åŽŸå­æ“ä½œ", "æ‰¹é‡å¤„ç†"]
            },
            {
                "title": "ðŸ“ æ–‡ä»¶å­˜å‚¨ (ClickZettaFileStore)",
                "description": "åŸºäºŽClickZetta Volumeçš„äºŒè¿›åˆ¶æ–‡ä»¶å­˜å‚¨",
                "example": "å­˜å‚¨åŽŸå§‹HTMLæ–‡ä»¶ã€å›¾ç‰‡ã€PDFç­‰",
                "benefits": ["äºŒè¿›åˆ¶æ”¯æŒ", "å¤§æ–‡ä»¶ä¼˜åŒ–", "ç‰ˆæœ¬ç®¡ç†"]
            },
            {
                "title": "ðŸ” å‘é‡å­˜å‚¨ (ClickZettaVectorStore)",
                "description": "æ”¯æŒè¯­ä¹‰æœç´¢çš„å‘é‡æ•°æ®åº“åŠŸèƒ½",
                "example": "åŸºäºŽå†…å®¹ç›¸ä¼¼æ€§çš„æ™ºèƒ½æ–‡æ¡£æ£€ç´¢",
                "benefits": ["è¯­ä¹‰ç†è§£", "ç›¸ä¼¼æ€§æœç´¢", "AIé©±åŠ¨"]
            }
        ]

        for i, feature in enumerate(features):
            with st.expander(f"{feature['title']}", expanded=i==0):
                st.write(feature['description'])
                st.write(f"**ç¤ºä¾‹ç”¨é€”**: {feature['example']}")

                col1, col2 = st.columns([2, 1])
                with col1:
                    st.write("**ä¸»è¦ä¼˜åŠ¿**:")
                    for benefit in feature['benefits']:
                        st.write(f"- {benefit}")

                with col2:
                    if i == 0:  # æ–‡æ¡£å­˜å‚¨
                        st.code('''
doc_store.store_document(
    key=url_hash,
    content=page_content,
    metadata={
        "title": title,
        "url": url,
        "crawled_at": timestamp
    }
)''')
                    elif i == 1:  # é”®å€¼å­˜å‚¨
                        st.code('''
cache_store.mset([
    ("status:123", b"completed"),
    ("hash:123", content_hash),
    ("time:123", timestamp)
])''')
                    elif i == 2:  # æ–‡ä»¶å­˜å‚¨
                        st.code('''
file_store.store_file(
    key="page.html",
    content=raw_html,
    content_type="text/html"
)''')
                    else:  # å‘é‡å­˜å‚¨
                        st.code('''
vector_store.add_documents([
    Document(
        page_content=content,
        metadata=metadata
    )
])''')

        st.markdown("""
        ### ðŸš€ æŠ€æœ¯ä¼˜åŠ¿

        1. **ç»Ÿä¸€å¹³å°**: æ‰€æœ‰å­˜å‚¨éœ€æ±‚åœ¨ä¸€ä¸ªClickZettaå¹³å°è§£å†³
        2. **ACIDäº‹åŠ¡**: ç¡®ä¿æ•°æ®ä¸€è‡´æ€§å’Œå¯é æ€§
        3. **é«˜æ€§èƒ½**: ClickZettaçš„äº‘åŽŸç”Ÿæž¶æž„æä¾›10å€æ€§èƒ½æå‡
        4. **æ˜“æ‰©å±•**: ä»ŽGBåˆ°PBçº§æ•°æ®çš„æ— ç¼æ‰©å±•
        5. **SQLå…¼å®¹**: ç†Ÿæ‚‰çš„SQLæŸ¥è¯¢è¯­æ³•
        6. **AIå°±ç»ª**: åŽŸç”Ÿæ”¯æŒå‘é‡æœç´¢å’Œæœºå™¨å­¦ä¹ 
        """)

        st.markdown("""
        ### ðŸ“ˆ å®žé™…åº”ç”¨åœºæ™¯

        - **ä¼ä¸šçŸ¥è¯†åº“**: çˆ¬å–å¹¶å­˜å‚¨å†…éƒ¨æ–‡æ¡£ã€æ”¿ç­–ã€æµç¨‹
        - **ç«žå“åˆ†æž**: ç›‘æŽ§ç«žäº‰å¯¹æ‰‹ç½‘ç«™å†…å®¹å˜åŒ–
        - **æ–°é—»èšåˆ**: æ”¶é›†å¤šæºæ–°é—»è¿›è¡Œåˆ†æžå’ŒæŽ¨è
        - **å­¦æœ¯ç ”ç©¶**: æ”¶é›†è®ºæ–‡ã€æŠ¥å‘Šè¿›è¡Œæ–‡çŒ®ç»¼è¿°
        - **ç”µå•†ç›‘æŽ§**: è·Ÿè¸ªäº§å“ä»·æ ¼ã€è¯„ä»·ã€åº“å­˜å˜åŒ–
        """)

if __name__ == "__main__":
    main()